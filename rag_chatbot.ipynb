{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: pinecone-client 6.0.0\n",
      "Uninstalling pinecone-client-6.0.0:\n",
      "  Successfully uninstalled pinecone-client-6.0.0\n",
      "Found existing installation: pinecone 7.3.0\n",
      "Uninstalling pinecone-7.3.0:\n",
      "  Successfully uninstalled pinecone-7.3.0\n",
      "Collecting pinecone\n",
      "  Downloading pinecone-7.3.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: langchain-pinecone in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (0.2.11)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from pinecone) (2025.8.3)\n",
      "Requirement already satisfied: pinecone-plugin-assistant<2.0.0,>=1.6.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from pinecone) (1.7.0)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from pinecone) (0.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from pinecone) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from pinecone) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from pinecone) (2.5.0)\n",
      "Requirement already satisfied: packaging<25.0,>=24.2 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (24.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.7)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.34 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-pinecone) (0.3.74)\n",
      "Requirement already satisfied: numpy>=1.26.4 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-pinecone) (2.3.2)\n",
      "Requirement already satisfied: langchain-tests<1.0.0,>=0.3.7 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-pinecone) (0.3.20)\n",
      "Requirement already satisfied: langchain-openai>=0.3.11 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-pinecone) (0.3.29)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (0.4.13)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (6.0.2)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (2.11.7)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (2.1)\n",
      "Requirement already satisfied: pytest<9,>=7 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (8.4.1)\n",
      "Requirement already satisfied: pytest-asyncio<1,>=0.20 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (0.26.0)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (0.28.1)\n",
      "Requirement already satisfied: syrupy<5,>=4 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (4.9.1)\n",
      "Requirement already satisfied: pytest-socket<1,>=0.6.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (0.7.0)\n",
      "Requirement already satisfied: pytest-benchmark in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (5.1.0)\n",
      "Requirement already satisfied: pytest-codspeed in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (4.0.0)\n",
      "Requirement already satisfied: pytest-recording in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (0.13.4)\n",
      "Requirement already satisfied: vcrpy>=7.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (7.0.0)\n",
      "Requirement already satisfied: anyio in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (0.16.0)\n",
      "Requirement already satisfied: aiohttp>=3.9.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (3.12.15)\n",
      "Requirement already satisfied: aiohttp-retry<3.0.0,>=2.9.1 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (2.9.1)\n",
      "Requirement already satisfied: iniconfig>=1 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (1.5.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (2.19.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (1.20.1)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-openai>=0.3.11->langchain-pinecone) (1.99.6)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-openai>=0.3.11->langchain-pinecone) (0.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai>=0.3.11->langchain-pinecone) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai>=0.3.11->langchain-pinecone) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai>=0.3.11->langchain-pinecone) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai>=0.3.11->langchain-pinecone) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (0.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from tiktoken<1,>=0.7->langchain-openai>=0.3.11->langchain-pinecone) (2025.7.34)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (3.11.1)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (0.23.0)\n",
      "Requirement already satisfied: six>=1.5 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
      "Requirement already satisfied: wrapt in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from vcrpy>=7.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (1.17.2)\n",
      "Requirement already satisfied: py-cpuinfo in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from pytest-benchmark->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (9.0.0)\n",
      "Requirement already satisfied: cffi>=1.17.1 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (1.17.1)\n",
      "Requirement already satisfied: rich>=13.8.1 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (13.9.4)\n",
      "Requirement already satisfied: pycparser in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from cffi>=1.17.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (2.21)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from rich>=13.8.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (2.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich>=13.8.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (0.1.0)\n",
      "Downloading pinecone-7.3.0-py3-none-any.whl (587 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.6/587.6 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pinecone\n",
      "Successfully installed pinecone-7.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y pinecone-client pinecone\n",
    "!pip install --no-cache-dir -U pinecone langchain-pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (0.1.1)\n",
      "Collecting langchain\n",
      "  Using cached langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: langchain-core in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (0.3.74)\n",
      "Requirement already satisfied: langchain-community in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (0.0.20)\n",
      "Collecting langchain-community\n",
      "  Using cached langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: langchain-text-splitters in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (0.3.9)\n",
      "Collecting langchain-huggingface\n",
      "  Using cached langchain_huggingface-0.3.1-py3-none-any.whl.metadata (996 bytes)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain) (0.4.13)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain) (2.0.42)\n",
      "Requirement already satisfied: requests<3,>=2 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-core) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-core) (4.12.2)\n",
      "Requirement already satisfied: packaging>=23.2 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-core) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-community) (3.12.15)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-community) (0.6.7)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Using cached pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Using cached httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=2.1.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-community) (2.3.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-huggingface) (0.21.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.33.4 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-huggingface) (0.34.4)\n",
      "Requirement already satisfied: filelock in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2025.7.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (1.1.7)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
      "Using cached langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
      "Using cached langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
      "Using cached httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
      "Using cached pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
      "Using cached langchain_huggingface-0.3.1-py3-none-any.whl (27 kB)\n",
      "Installing collected packages: httpx-sse, pydantic-settings, langchain-huggingface, langchain, langchain-community\n",
      "\u001b[2K  Attempting uninstall: langchain\n",
      "\u001b[2K    Found existing installation: langchain 0.1.1\n",
      "\u001b[2K    Uninstalling langchain-0.1.1:\n",
      "\u001b[2K      Successfully uninstalled langchain-0.1.1\n",
      "\u001b[2K  Attempting uninstall: langchain-community[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [langchain]\n",
      "\u001b[2K    Found existing installation: langchain-community 0.0.20━━━\u001b[0m \u001b[32m3/5\u001b[0m [langchain]\n",
      "\u001b[2K    Uninstalling langchain-community-0.0.20:90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [langchain]\n",
      "\u001b[2K      Successfully uninstalled langchain-community-0.0.20━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [langchain]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [langchain-community]ngchain-community]\n",
      "\u001b[1A\u001b[2KSuccessfully installed httpx-sse-0.4.1 langchain-0.3.27 langchain-community-0.3.27 langchain-huggingface-0.3.1 pydantic-settings-2.10.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain langchain-core langchain-community langchain-text-splitters langchain-huggingface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pinecone version: 7.3.0\n"
     ]
    }
   ],
   "source": [
    "import pinecone, sys\n",
    "print(\"pinecone version:\", getattr(pinecone, \"__version__\", \"unknown\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (0.34.4)\n",
      "Requirement already satisfied: langchain-community>=0.2.10 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-core>=0.2.38 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (0.3.74)\n",
      "Requirement already satisfied: filelock in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from huggingface_hub>=0.25.0) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from huggingface_hub>=0.25.0) (2025.7.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from huggingface_hub>=0.25.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from huggingface_hub>=0.25.0) (6.0.2)\n",
      "Requirement already satisfied: requests in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from huggingface_hub>=0.25.0) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from huggingface_hub>=0.25.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from huggingface_hub>=0.25.0) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from huggingface_hub>=0.25.0) (1.1.7)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-community>=0.2.10) (0.3.27)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-community>=0.2.10) (2.0.42)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-community>=0.2.10) (3.12.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-community>=0.2.10) (8.5.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-community>=0.2.10) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-community>=0.2.10) (2.10.1)\n",
      "Requirement already satisfied: langsmith>=0.1.125 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-community>=0.2.10) (0.4.13)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-community>=0.2.10) (0.4.1)\n",
      "Requirement already satisfied: numpy>=2.1.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-community>=0.2.10) (2.3.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-core>=0.2.38) (1.33)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-core>=0.2.38) (2.11.7)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.10) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.10) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.10) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.10) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.10) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.10) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.10) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community>=0.2.10) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community>=0.2.10) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.2.38) (2.1)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain<1.0.0,>=0.3.26->langchain-community>=0.2.10) (0.3.9)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from pydantic>=2.7.4->langchain-core>=0.2.38) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from pydantic>=2.7.4->langchain-core>=0.2.38) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from pydantic>=2.7.4->langchain-core>=0.2.38) (0.4.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community>=0.2.10) (1.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from SQLAlchemy<3,>=1.4->langchain-community>=0.2.10) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community>=0.2.10) (1.1.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langsmith>=0.1.125->langchain-community>=0.2.10) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langsmith>=0.1.125->langchain-community>=0.2.10) (3.11.1)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langsmith>=0.1.125->langchain-community>=0.2.10) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langsmith>=0.1.125->langchain-community>=0.2.10) (0.23.0)\n",
      "Requirement already satisfied: anyio in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community>=0.2.10) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community>=0.2.10) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community>=0.2.10) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community>=0.2.10) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --no-cache-dir \\\n",
    "  \"huggingface_hub>=0.25.0\" \\\n",
    "  \"langchain-community>=0.2.10\" \\\n",
    "  \"langchain-core>=0.2.38\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface_hub: 0.34.4\n",
      "langchain_community: 0.3.27\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub, langchain_community\n",
    "print(\"huggingface_hub:\", huggingface_hub.__version__)\n",
    "print(\"langchain_community:\", langchain_community.__version__)\n",
    "# Expect: huggingface_hub >= 0.25.x, langchain_community >= 0.2.10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinecone key loaded: True\n",
      "HF token loaded: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Map either var name to what our code expects\n",
    "if os.getenv(\"HUGGINGFACEHUB_API_TOKEN\") is None and os.getenv(\"HUGGINGFACEHUB_API_KEY\"):\n",
    "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = os.getenv(\"HUGGINGFACEHUB_API_KEY\")\n",
    "\n",
    "pinecone_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "hf_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "print(\"Pinecone key loaded:\", bool(pinecone_key))\n",
    "print(\"HF token loaded:\", bool(hf_token))\n",
    "assert pinecone_key and hf_token, \"Missing PINECONE_API_KEY or HF token\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 doc(s), split into 7 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Load & split data\n",
    "loader = TextLoader(\"./horoscope.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=4)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(f\"Loaded {len(documents)} doc(s), split into {len(docs)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dim: 768\n"
     ]
    }
   ],
   "source": [
    "# Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "dim = len(embeddings.embed_query(\"ping\"))\n",
    "print(\"Embedding dim:\", dim)  # should print 768 for this model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to existing index with ~7 vectors.\n",
      "Vectorstore ready ✅\n"
     ]
    }
   ],
   "source": [
    "# Pinecone: index + vector store\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "INDEX_NAME = \"langchain-demo\"     # lowercase / digits / dashes only\n",
    "CLOUD, REGION = \"aws\", \"us-east-1\"  # free tier-friendly region\n",
    "\n",
    "# Create index if missing\n",
    "if INDEX_NAME not in [i[\"name\"] for i in pc.list_indexes()]:\n",
    "    pc.create_index(\n",
    "        name=INDEX_NAME,\n",
    "        dimension=dim,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=CLOUD, region=REGION),\n",
    "    )\n",
    "\n",
    "# If index is empty, upsert; otherwise just connect\n",
    "idx = pc.Index(INDEX_NAME)\n",
    "stats = idx.describe_index_stats()\n",
    "total = stats.get(\"total_vector_count\", 0)\n",
    "\n",
    "if total == 0:\n",
    "    vectorstore = PineconeVectorStore.from_documents(docs, embedding=embeddings, index_name=INDEX_NAME)\n",
    "    print(\"Upserted docs to Pinecone.\")\n",
    "else:\n",
    "    vectorstore = PineconeVectorStore(index_name=INDEX_NAME, embedding=embeddings)\n",
    "    print(f\"Connected to existing index with ~{total} vectors.\")\n",
    "\n",
    "print(\"Vectorstore ready ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] When hardworking Saturn shifts into ambitious, competitive Aries at the end of March, you may feel renewed confidence in your career. Embracing leadership roles can boost your morale and help you rega...\n",
      "\n",
      "[2] Love takes you to new heights this year, Sag. You’re all about seeking truth and revealing it. As Venus, the goddess of love, spends most of January in the dreamy, creative sign of Pisces, you’re unde...\n",
      "\n",
      "[3] Sagittarius Horoscope of 2025\n",
      "\n",
      "As the fiery Archer, your outgoing and adventurous nature is powered by Jupiter, your optimistic planet. This year, Jupiter joins forces with Gemini and Cancer in your s...\n"
     ]
    }
   ],
   "source": [
    "res = vectorstore.similarity_search(\"lucky number for leo\", k=3)\n",
    "for i, r in enumerate(res, 1):\n",
    "    print(f\"\\n[{i}] {r.page_content[:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever ready ✅\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "print(\"Retriever ready ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat LLM ready ✅\n"
     ]
    }
   ],
   "source": [
    "chat_llm = ChatHuggingFace(\n",
    "    llm=HuggingFaceEndpoint(\n",
    "        repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "        task=\"conversational\",        # IMPORTANT for this model\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        max_new_tokens=256,\n",
    "        huggingfacehub_api_token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Chat LLM ready ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The capital of France is Paris. Known for its iconic landmarks like the Eiffel Tower, Louvre Museum, Notre-Dame Cathedral, and sophisticated cafes, Paris is one of the most popular tourist destinations in the world. It's renowned for its art, fashion, gastronomy, and culture, making it a city that truly has something for everyone.\n"
     ]
    }
   ],
   "source": [
    "resp = chat_llm.invoke(\"Answer briefly: What's the capital of France?\")\n",
    "print(getattr(resp, \"content\", resp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a fortune teller. Use the provided context to answer. \"\n",
    "     \"If you don't know, say you don't know. Keep the answer within 2 sentences and concise.\"),\n",
    "    (\"human\", \"Context:\\n{context}\\n\\nQuestion: {question}\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chain ready ✅\n"
     ]
    }
   ],
   "source": [
    "# Compose: retrieve -> format -> prompt -> chat -> extract content\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | RunnableLambda(format_docs),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | chat_llm\n",
    "    | RunnableLambda(lambda msg: getattr(msg, \"content\", str(msg)))\n",
    ")\n",
    "\n",
    "print(\"RAG chain ready ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I don't have real-time information or personal data, so I can't provide specific predictions for this week for Leo. However, based on the provided context, there is no Leo-specific information available, so it would be best for Leo to follow the general guidance: stay aware of financial shifts, consider a long-term plan, and consult a professional if needed. No career or love horoscope was given for Leo this week.\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain.invoke(\"What does this week look like for Leo?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In 2025, you'll experience growth and transformation in your relationships due to Jupiter's influence, but be mindful of Mercury retrograde periods disrupting your travel plans. Financially, stay aware of global events and innovations, considering a long-term financial plan. Your love life may see deeper connections and fresh perspectives towards the end of the year.\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain.invoke(\"How is my life going to be iin 2025?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In 2\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "from langchain_huggingface import (\n",
    "    HuggingFaceEmbeddings,\n",
    "    ChatHuggingFace,\n",
    "    HuggingFaceEndpoint,\n",
    ")\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "class ChatBot:\n",
    "    def __init__(self, index_name: str = \"langchain-demo\", cloud: str = \"aws\", region: str = \"us-east-1\"):\n",
    "        load_dotenv()\n",
    "\n",
    "        # HF env compatibility\n",
    "        if os.getenv(\"HUGGINGFACEHUB_API_TOKEN\") is None and os.getenv(\"HUGGINGFACEHUB_API_KEY\"):\n",
    "            os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = os.getenv(\"HUGGINGFACEHUB_API_KEY\")\n",
    "\n",
    "        pinecone_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "        hf_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "        if not pinecone_key or not hf_token:\n",
    "            raise RuntimeError(\"Missing PINECONE_API_KEY or HUGGINGFACEHUB_API_TOKEN in environment.\")\n",
    "\n",
    "        # ------- Load & split docs -------\n",
    "        if not os.path.exists(\"./horoscope.txt\"):\n",
    "            raise FileNotFoundError(\"Couldn't find './horoscope.txt'. Make sure the file exists.\")\n",
    "\n",
    "        loader = TextLoader(\"./horoscope.txt\")\n",
    "        documents = loader.load()\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=4)\n",
    "        self.docs = text_splitter.split_documents(documents)\n",
    "\n",
    "        # ------- Embeddings -------\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "        dim = len(self.embeddings.embed_query(\"ping\"))  # expected 768\n",
    "\n",
    "        # ------- Pinecone setup -------\n",
    "        pc = Pinecone(api_key=pinecone_key)\n",
    "        self.index_name = index_name\n",
    "\n",
    "        # Handle both dict and object forms from pc.list_indexes() across client versions\n",
    "        def _idx_name(x):\n",
    "            return x.name if hasattr(x, \"name\") else (x.get(\"name\") if isinstance(x, dict) else None)\n",
    "\n",
    "        existing = {_idx_name(i) for i in pc.list_indexes()}\n",
    "        if self.index_name not in existing:\n",
    "            pc.create_index(\n",
    "                name=self.index_name,\n",
    "                dimension=dim,\n",
    "                metric=\"cosine\",\n",
    "                spec=ServerlessSpec(cloud=cloud, region=region),\n",
    "            )\n",
    "\n",
    "        idx = pc.Index(self.index_name)\n",
    "        stats = idx.describe_index_stats()\n",
    "\n",
    "        # Total vectors across namespaces\n",
    "        namespaces = stats.get(\"namespaces\", {}) or {}\n",
    "        total = sum(ns.get(\"vector_count\", 0) for ns in namespaces.values())\n",
    "\n",
    "        # ------- Vector store -------\n",
    "        if total == 0:\n",
    "            self.vectorstore = PineconeVectorStore.from_documents(\n",
    "                self.docs, embedding=self.embeddings, index_name=self.index_name\n",
    "            )\n",
    "        else:\n",
    "            self.vectorstore = PineconeVectorStore(index_name=self.index_name, embedding=self.embeddings)\n",
    "\n",
    "        # Optional quick sanity check (safe even if no results)\n",
    "        try:\n",
    "            res = self.vectorstore.similarity_search(\"lucky number for leo\", k=3)\n",
    "            for i, r in enumerate(res, 1):\n",
    "                # print(f\"[{i}] {r.page_content[:200]}...\")\n",
    "                pass\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        self.retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "        # ------- LLM -------\n",
    "        self.chat_llm = ChatHuggingFace(\n",
    "            llm=HuggingFaceEndpoint(\n",
    "                repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "                task=\"text-generation\",\n",
    "                temperature=0.7,\n",
    "                top_k=50,\n",
    "                max_new_tokens=256,\n",
    "                do_sample=True,\n",
    "                return_full_text=False,\n",
    "                huggingfacehub_api_token=hf_token,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # ------- Prompt -------\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\",\n",
    "             \"You are a fortune teller. Use the provided context to answer. \"\n",
    "             \"If you don't know, say you don't know. Keep the answer within 2 sentences and concise.\"),\n",
    "            (\"human\", \"Context:\\n{context}\\n\\nQuestion: {question}\")\n",
    "        ])\n",
    "\n",
    "        # Keep as a nested function to avoid capturing self in RunnableLambda\n",
    "        def format_docs(docs):\n",
    "            return \"\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "        # ------- RAG chain -------\n",
    "        self.rag_chain = (\n",
    "            {\n",
    "                \"context\": self.retriever | RunnableLambda(format_docs),\n",
    "                \"question\": RunnablePassthrough(),\n",
    "            }\n",
    "            | prompt\n",
    "            | self.chat_llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bot = ChatBot()\n",
    "    user_q = input(\"Ask me anything: \")\n",
    "    result = bot.rag_chain.invoke(user_q)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reranker (BAAI/bge-reranker-base)…\n",
      "Advanced retriever ready (multi-query + rerank + compression) ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2581454/3101785251.py:41: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = advanced_retriever.get_relevant_documents(q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved: 5 docs\n",
      "Sagittarius Horoscope of 2025\n",
      "\n",
      "As the fiery Archer, your outgoing and adventurous nature is powered by Jupiter, your optimistic planet. This year, Jupiter joins forces with Gemini and Cancer in your s…\n"
     ]
    }
   ],
   "source": [
    "import os, time, logging\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "assert \"vectorstore\" in globals(), \"Run earlier cells to create `vectorstore` first.\"\n",
    "logging.getLogger(\"huggingface_hub\").setLevel(logging.WARNING)\n",
    "\n",
    "# 1) Query rewriter LLM (fast, deterministic)\n",
    "rewriter_llm = ChatHuggingFace(\n",
    "    llm=HuggingFaceEndpoint(\n",
    "        repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "        task=\"text-generation\",\n",
    "        temperature=0.2,\n",
    "        do_sample=False,\n",
    "        top_k=None,\n",
    "        max_new_tokens=96,\n",
    "        return_full_text=False,\n",
    "    )\n",
    ")\n",
    "# 2) Base retriever → MultiQuery\n",
    "base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "mqr = MultiQueryRetriever.from_llm(retriever=base_retriever, llm=rewriter_llm)\n",
    "\n",
    "# 3) Cross-encoder reranker (IMPORTANT: use model=, not cross_encoder=)\n",
    "print(\"Loading reranker (BAAI/bge-reranker-base)…\")\n",
    "cross_encoder = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-base\")\n",
    "reranker = CrossEncoderReranker(model=cross_encoder, top_n=5)  # <-- fix here\n",
    "\n",
    "# 4) Final advanced retriever\n",
    "advanced_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=reranker,\n",
    "    base_retriever=mqr\n",
    ")\n",
    "print(\"Advanced retriever ready (multi-query + rerank + compression) ✅\")\n",
    "\n",
    "# 5) Smoke test\n",
    "q = \"What can Gemini expect this week?\"\n",
    "docs = advanced_retriever.get_relevant_documents(q)\n",
    "print(\"Retrieved:\", len(docs), \"docs\")\n",
    "print(docs[0].page_content[:200] + \"…\" if docs else \"No docs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers: context formatting + citations\n",
    "from typing import List\n",
    "from langchain_core.documents import Document  # <-- add this import\n",
    "\n",
    "def format_context(docs: List[Document]) -> str:\n",
    "    # join only non-empty page_content\n",
    "    return \"\\n\\n\".join(\n",
    "        getattr(d, \"page_content\", \"\") for d in docs if getattr(d, \"page_content\", \"\")\n",
    "    )\n",
    "\n",
    "def extract_citations(docs: List[Document]) -> List[str]:\n",
    "    cites = []\n",
    "    for i, d in enumerate(docs, 1):\n",
    "        md = (getattr(d, \"metadata\", {}) or {})\n",
    "        src = md.get(\"source\") or md.get(\"file\") or \"horoscope.txt\"\n",
    "        cites.append(f\"[{i}] {src}\")\n",
    "    return cites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools (example): lucky number + now\n",
    "from langchain_core.tools import tool\n",
    "import hashlib, datetime\n",
    "\n",
    "@tool(\"lucky_number\")\n",
    "def lucky_number(name_or_sign: str) -> str:\n",
    "    \"\"\"\n",
    "    Deterministic 'lucky number' (1-9) from a name or zodiac sign.\n",
    "    \"\"\"\n",
    "    h = int(hashlib.md5(name_or_sign.strip().lower().encode(\"utf-8\")).hexdigest(), 16)\n",
    "    num = (h % 9) + 1\n",
    "    return f\"Lucky number for '{name_or_sign}': {num}\"\n",
    "\n",
    "@tool(\"now\")\n",
    "def now(_: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Current date/time (ISO format).\n",
    "    \"\"\"\n",
    "    return datetime.datetime.now().isoformat(timespec=\"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangGraph app compiled ✅\n"
     ]
    }
   ],
   "source": [
    "# LangGraph pipeline: route → tools/RAG → grade → generate (fixed imports)\n",
    "from typing import TypedDict, List, Literal, Optional\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# --- sanity checks ---\n",
    "assert \"advanced_retriever\" in globals(), \"Build advanced_retriever first\"\n",
    "assert \"chat_llm\" in globals(), \"Build chat_llm first\"\n",
    "assert \"lucky_number\" in globals() and \"now\" in globals(), \"Define tools first\"\n",
    "assert \"extract_citations\" in globals() and \"format_context\" in globals(), \"Add helper funcs first\"\n",
    "\n",
    "# (Optional) sign extractor to make tool output cleaner\n",
    "SIGNS = [\n",
    "    \"aries\",\"taurus\",\"gemini\",\"cancer\",\"leo\",\"virgo\",\n",
    "    \"libra\",\"scorpio\",\"sagittarius\",\"capricorn\",\"aquarius\",\"pisces\"\n",
    "]\n",
    "def _extract_sign_or_name(q: str) -> str:\n",
    "    t = (q or \"\").lower()\n",
    "    for s in SIGNS:\n",
    "        if s in t:\n",
    "            return s.title()\n",
    "    return q.strip()\n",
    "\n",
    "class RAGState(TypedDict, total=False):\n",
    "    session_id: str\n",
    "    question: str\n",
    "    history: List                 # for memory wrapper\n",
    "    route: Literal[\"TOOLS\", \"RAG\"]\n",
    "    docs: List[Document]\n",
    "    citations: List[str]\n",
    "    tool_result: Optional[str]\n",
    "    grounded: bool\n",
    "    answer: str\n",
    "\n",
    "TOOL_KEYWORDS = (\"lucky number\", \"lucky\", \"today\", \"date\", \"time\", \"now\")\n",
    "\n",
    "def route_node(state: RAGState) -> RAGState:\n",
    "    q = (state.get(\"question\") or \"\").lower()\n",
    "    return {**state, \"route\": \"TOOLS\" if any(k in q for k in TOOL_KEYWORDS) else \"RAG\"}\n",
    "\n",
    "def tools_node(state: RAGState) -> RAGState:\n",
    "    q = state[\"question\"]\n",
    "    if \"lucky\" in q.lower():\n",
    "        target = _extract_sign_or_name(q)\n",
    "        result = lucky_number.invoke(target)   # e.g., \"Lucky number for 'Gemini': 8\"\n",
    "    else:\n",
    "        result = now.invoke(\"\")\n",
    "    # Short-circuit: finalize answer here (skip LLM)\n",
    "    return {**state, \"tool_result\": result, \"answer\": result,\n",
    "            \"docs\": [], \"citations\": [], \"grounded\": True}\n",
    "\n",
    "def retrieve_node(state: RAGState) -> RAGState:\n",
    "    docs = advanced_retriever.invoke(state[\"question\"])   # new API\n",
    "    return {**state, \"docs\": docs, \"citations\": extract_citations(docs)}\n",
    "\n",
    "def grade_node(state: RAGState) -> RAGState:\n",
    "    return {**state, \"grounded\": len(state.get(\"docs\") or []) > 0}\n",
    "\n",
    "# Tight prompt: only cite provided citations; if none, cite nothing.\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a concise fortune teller. Use ONLY the provided context. \"\n",
    "     \"Rules: (1) If context is empty or irrelevant, say you don't know. \"\n",
    "     \"(2) Cite ONLY items listed under 'Citations:'. \"\n",
    "     \"(3) If 'Citations:' is empty, do not cite anything.\"),\n",
    "    MessagesPlaceholder(\"history\"),\n",
    "    (\"human\", \"Context:\\n{context}\\n\\nCitations:\\n{citations}\\n\\nQuestion: {question}\")\n",
    "])\n",
    "generator = rag_prompt | chat_llm | StrOutputParser()\n",
    "\n",
    "def generate_node(state: RAGState) -> RAGState:\n",
    "    # If tools already answered, do nothing\n",
    "    if state.get(\"tool_result\"):\n",
    "        return state\n",
    "    context = format_context(state.get(\"docs\", [])) if state.get(\"docs\") else \"\"\n",
    "    citations_text = \" \".join(state.get(\"citations\", []))\n",
    "    answer = generator.invoke({\n",
    "        \"history\": state.get(\"history\", []),\n",
    "        \"context\": context,\n",
    "        \"citations\": citations_text,\n",
    "        \"question\": state[\"question\"],\n",
    "    })\n",
    "    return {**state, \"answer\": answer}\n",
    "\n",
    "graph = StateGraph(RAGState)\n",
    "graph.add_node(\"route\", route_node)\n",
    "graph.add_node(\"tools\", tools_node)\n",
    "graph.add_node(\"retrieve\", retrieve_node)\n",
    "graph.add_node(\"grade\", grade_node)\n",
    "graph.add_node(\"generate\", generate_node)\n",
    "\n",
    "graph.add_edge(START, \"route\")\n",
    "graph.add_conditional_edges(\"route\", lambda s: \"tools\" if s[\"route\"] == \"TOOLS\" else \"retrieve\")\n",
    "graph.add_edge(\"tools\", END)                  # <-- tools short-circuit to END\n",
    "graph.add_edge(\"retrieve\", \"grade\")\n",
    "graph.add_conditional_edges(\"grade\", lambda s: \"generate\" if s[\"grounded\"] else END)\n",
    "graph.add_edge(\"generate\", END)\n",
    "\n",
    "rag_app = graph.compile()\n",
    "print(\"LangGraph app compiled ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory wrapper ready ✅\n"
     ]
    }
   ],
   "source": [
    "# Make the graph return only the final answer string\n",
    "rag_app_answer = rag_app | (lambda state: state[\"answer\"])\n",
    "\n",
    "# Memory wrapper\n",
    "try:\n",
    "    from langchain_community.chat_message_histories import ChatMessageHistory as InMemoryChatMessageHistory\n",
    "except ImportError:\n",
    "    from langchain_community.chat_message_histories import InMemoryChatMessageHistory\n",
    "\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "_session_store = {}\n",
    "def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in _session_store:\n",
    "        _session_store[session_id] = InMemoryChatMessageHistory()\n",
    "    return _session_store[session_id]\n",
    "\n",
    "rag_app_with_memory = RunnableWithMessageHistory(\n",
    "    rag_app_answer,                 # <-- string output runnable\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",  # must match MessagesPlaceholder(\"history\")\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "print(\"Memory wrapper ready ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG path (uses your corpus):\n",
      " I don't have information specific to Gemini for this week, as the context provided refers to Sagittarius Horoscope of 2025, Sagittarius Career Horoscope, and Sagittarius Love Horoscope, without any weekly updates.\n",
      "\n",
      "Tools path (short-circuits to tool output):\n",
      "Lucky number for 'Gemini': 3\n"
     ]
    }
   ],
   "source": [
    "#Try it (RAG and Tools paths)\n",
    "cfg = {\"configurable\": {\"session_id\": \"demo-user-1\"}}\n",
    "\n",
    "print(\"RAG path (uses your corpus):\")\n",
    "ans1 = rag_app_with_memory.invoke({\"question\": \"What can Gemini expect this week?\"}, config=cfg)\n",
    "print(ans1)   # <-- string\n",
    "\n",
    "print(\"\\nTools path (short-circuits to tool output):\")\n",
    "ans2 = rag_app_with_memory.invoke({\"question\": \"What's my lucky number for Gemini?\"}, config=cfg)\n",
    "print(ans2)   # <-- string (no fake citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer_with_graph() ready ✅\n"
     ]
    }
   ],
   "source": [
    "# Helper for Streamlit to call the graph\n",
    "def answer_with_graph(user_text: str, session_id: str = \"web-user-1\") -> str:\n",
    "    cfg = {\"configurable\": {\"session_id\": session_id}}\n",
    "    state = {\"session_id\": session_id, \"question\": user_text}\n",
    "    out = rag_app_with_memory.invoke(state, config=cfg)\n",
    "    ans = out.get(\"answer\", \"\")\n",
    "    cites = out.get(\"citations\", [])\n",
    "    if cites:\n",
    "        ans = f\"{ans}\\n\\nSources: \" + \" \".join(cites)\n",
    "    return ans\n",
    "\n",
    "print(\"answer_with_graph() ready ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The capital of France is Paris. [1] https://www.worldometers.info/capitals/france-capital/\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "import os\n",
    "from typing import List, Optional, Literal, TypedDict\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Pinecone + Vector store\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# Embeddings / LLM\n",
    "from langchain_huggingface import (\n",
    "    HuggingFaceEmbeddings,\n",
    "    ChatHuggingFace,\n",
    "    HuggingFaceEndpoint,\n",
    ")\n",
    "\n",
    "# Data loading / splitting\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# LangChain core\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Advanced retrieval\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "# Tools\n",
    "from langchain_core.tools import tool\n",
    "import hashlib, datetime\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Memory (version-agnostic import)\n",
    "try:\n",
    "    # Newer LangChain\n",
    "    from langchain_community.chat_message_histories import ChatMessageHistory as InMemoryChatMessageHistory\n",
    "except ImportError:\n",
    "    # Older LangChain\n",
    "    from langchain_community.chat_message_histories import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "\n",
    "class ChatBot:\n",
    "    def __init__(\n",
    "        self,\n",
    "        index_name: str = \"langchain-demo\",\n",
    "        cloud: str = \"aws\",\n",
    "        region: str = \"us-east-1\",\n",
    "    ):\n",
    "        load_dotenv()\n",
    "\n",
    "        # HF env compatibility\n",
    "        if os.getenv(\"HUGGINGFACEHUB_API_TOKEN\") is None and os.getenv(\"HUGGINGFACEHUB_API_KEY\"):\n",
    "            os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = os.getenv(\"HUGGINGFACEHUB_API_KEY\")\n",
    "\n",
    "        pinecone_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "        hf_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "        if not pinecone_key or not hf_token:\n",
    "            raise RuntimeError(\"Missing PINECONE_API_KEY or HUGGINGFACEHUB_API_TOKEN in environment.\")\n",
    "\n",
    "        # ------- Load & split docs -------\n",
    "        if not os.path.exists(\"./horoscope.txt\"):\n",
    "            raise FileNotFoundError(\"Couldn't find './horoscope.txt'. Make sure the file exists.\")\n",
    "        loader = TextLoader(\"./horoscope.txt\")\n",
    "        documents = loader.load()\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=4)\n",
    "        docs = text_splitter.split_documents(documents)\n",
    "\n",
    "        # ------- Embeddings -------\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "        dim = len(self.embeddings.embed_query(\"ping\"))  # expected 768\n",
    "\n",
    "        # ------- Pinecone setup -------\n",
    "        pc = Pinecone(api_key=pinecone_key)\n",
    "        self.index_name = index_name\n",
    "\n",
    "        def _idx_name(x):\n",
    "            return x.name if hasattr(x, \"name\") else (x.get(\"name\") if isinstance(x, dict) else None)\n",
    "\n",
    "        existing = {_idx_name(i) for i in pc.list_indexes()}\n",
    "        if self.index_name not in existing:\n",
    "            pc.create_index(\n",
    "                name=self.index_name,\n",
    "                dimension=dim,\n",
    "                metric=\"cosine\",\n",
    "                spec=ServerlessSpec(cloud=cloud, region=region),\n",
    "            )\n",
    "\n",
    "        idx = pc.Index(self.index_name)\n",
    "        stats = idx.describe_index_stats()\n",
    "        namespaces = stats.get(\"namespaces\", {}) or {}\n",
    "        total = sum(ns.get(\"vector_count\", 0) for ns in namespaces.values()) if namespaces else stats.get(\"total_vector_count\", 0) or 0\n",
    "\n",
    "        if total == 0:\n",
    "            self.vectorstore = PineconeVectorStore.from_documents(docs, embedding=self.embeddings, index_name=self.index_name)\n",
    "        else:\n",
    "            self.vectorstore = PineconeVectorStore(index_name=self.index_name, embedding=self.embeddings)\n",
    "\n",
    "        # ------- LLMs -------\n",
    "        HF_TOKEN = hf_token\n",
    "\n",
    "        # Final answerer (fluent)\n",
    "        self.chat_llm = ChatHuggingFace(\n",
    "            llm=HuggingFaceEndpoint(\n",
    "                repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "                task=\"text-generation\",\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                max_new_tokens=256,\n",
    "                return_full_text=False,\n",
    "                huggingfacehub_api_token=HF_TOKEN,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Deterministic rewriter (can be same model; different decoding)\n",
    "        self.rewriter_llm = ChatHuggingFace(\n",
    "            llm=HuggingFaceEndpoint(\n",
    "                repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "                task=\"text-generation\",\n",
    "                temperature=0.2,\n",
    "                do_sample=False,\n",
    "                max_new_tokens=96,\n",
    "                return_full_text=False,\n",
    "                huggingfacehub_api_token=HF_TOKEN,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # ------- Advanced retriever (MultiQuery + Cross-Encoder rerank + Compression) -------\n",
    "        base_retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "        mqr = MultiQueryRetriever.from_llm(retriever=base_retriever, llm=self.rewriter_llm)\n",
    "\n",
    "        reranker_model_name = os.getenv(\"RERANKER_MODEL\", \"BAAI/bge-reranker-base\")  # switch to -large if you want\n",
    "        cross_encoder = HuggingFaceCrossEncoder(model_name=reranker_model_name)\n",
    "        reranker = CrossEncoderReranker(model=cross_encoder, top_n=5)\n",
    "\n",
    "        self.advanced_retriever = ContextualCompressionRetriever(\n",
    "            base_compressor=reranker,\n",
    "            base_retriever=mqr,\n",
    "        )\n",
    "\n",
    "        # ------- Helpers -------\n",
    "        self.SIGNS = [\n",
    "            \"aries\",\"taurus\",\"gemini\",\"cancer\",\"leo\",\"virgo\",\n",
    "            \"libra\",\"scorpio\",\"sagittarius\",\"capricorn\",\"aquarius\",\"pisces\"\n",
    "        ]\n",
    "\n",
    "        def extract_sign_from_text(text: str) -> Optional[str]:\n",
    "            t = (text or \"\").lower()\n",
    "            for s in self.SIGNS:\n",
    "                if s in t:\n",
    "                    return s\n",
    "            return None\n",
    "\n",
    "        def filter_docs_by_sign(docs_list: List[Document], sign: Optional[str]) -> List[Document]:\n",
    "            if not sign:\n",
    "                return docs_list\n",
    "            s = sign.lower()\n",
    "            return [d for d in docs_list if s in (d.page_content or \"\").lower()]\n",
    "\n",
    "        def extract_citations(docs_list: List[Document]) -> List[str]:\n",
    "            cites = []\n",
    "            for i, d in enumerate(docs_list, 1):\n",
    "                md = (getattr(d, \"metadata\", {}) or {})\n",
    "                src = md.get(\"source\") or md.get(\"file\") or \"horoscope.txt\"\n",
    "                cites.append(f\"[{i}] {src}\")\n",
    "            return cites\n",
    "\n",
    "        def format_context(docs_list: List[Document]) -> str:\n",
    "            return \"\\n\\n\".join(\n",
    "                getattr(d, \"page_content\", \"\") for d in docs_list if getattr(d, \"page_content\", \"\")\n",
    "            )\n",
    "\n",
    "        # ------- Tools -------\n",
    "        @tool(\"lucky_number\")\n",
    "        def lucky_number(name_or_sign: str) -> str:\n",
    "            \"\"\"Deterministic 'lucky number' (1-9) from a name or zodiac sign.\"\"\"\n",
    "            h = int(hashlib.md5(name_or_sign.strip().lower().encode(\"utf-8\")).hexdigest(), 16)\n",
    "            num = (h % 9) + 1\n",
    "            return f\"Lucky number for '{name_or_sign}': {num}\"\n",
    "\n",
    "        @tool(\"now\")\n",
    "        def now(_: str = \"\") -> str:\n",
    "            \"\"\"Current date/time (ISO format).\"\"\"\n",
    "            return datetime.datetime.now().isoformat(timespec=\"seconds\")\n",
    "\n",
    "        self.lucky_number = lucky_number\n",
    "        self.now = now\n",
    "\n",
    "        # ------- LangGraph (sign-aware): route → tools/RAG → grade → fallback/generate -------\n",
    "        class RAGState(TypedDict, total=False):\n",
    "            session_id: str\n",
    "            question: str\n",
    "            history: List\n",
    "            route: Literal[\"TOOLS\", \"RAG\"]\n",
    "            target_sign: Optional[str]\n",
    "            docs: List[Document]\n",
    "            citations: List[str]\n",
    "            tool_result: Optional[str]\n",
    "            grounded: bool\n",
    "            answer: str\n",
    "\n",
    "        TOOL_KEYWORDS = (\"lucky number\", \"lucky\", \"today\", \"date\", \"time\", \"now\")\n",
    "\n",
    "        def route_node(state: RAGState) -> RAGState:\n",
    "            q = (state.get(\"question\") or \"\")\n",
    "            route = \"TOOLS\" if any(k in q.lower() for k in TOOL_KEYWORDS) else \"RAG\"\n",
    "            return {**state, \"route\": route, \"target_sign\": extract_sign_from_text(q)}\n",
    "\n",
    "        def tools_node(state: RAGState) -> RAGState:\n",
    "            q = state[\"question\"]\n",
    "            if \"lucky\" in q.lower():\n",
    "                target = extract_sign_from_text(q) or q.strip()\n",
    "                result = self.lucky_number.invoke(target.title())\n",
    "            else:\n",
    "                result = self.now.invoke(\"\")\n",
    "            return {**state, \"tool_result\": result, \"answer\": result, \"docs\": [], \"citations\": [], \"grounded\": True}\n",
    "\n",
    "        def retrieve_node(state: RAGState) -> RAGState:\n",
    "            docs_list = self.advanced_retriever.invoke(state[\"question\"])  # new API\n",
    "            sign = state.get(\"target_sign\")\n",
    "            if sign:\n",
    "                docs_list = filter_docs_by_sign(docs_list, sign)\n",
    "            return {**state, \"docs\": docs_list, \"citations\": extract_citations(docs_list)}\n",
    "\n",
    "        def grade_node(state: RAGState) -> RAGState:\n",
    "            grounded = len(state.get(\"docs\") or []) > 0\n",
    "            return {**state, \"grounded\": grounded}\n",
    "\n",
    "        def fallback_node(state: RAGState) -> RAGState:\n",
    "            sign = state.get(\"target_sign\")\n",
    "            if sign:\n",
    "                msg = f\"I don't have content for {sign.title()} yet. Please add it to the corpus.\"\n",
    "            else:\n",
    "                msg = \"I don't know yet. Please add relevant content to the corpus.\"\n",
    "            return {**state, \"answer\": msg}\n",
    "\n",
    "        rag_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\",\n",
    "             \"You are a helpful, concise fortune teller. Use ONLY the provided context and tools. \"\n",
    "             \"If you don't know, say you don't know. Keep answers within 2 short sentences.\"),\n",
    "            MessagesPlaceholder(\"history\"),\n",
    "            (\"human\",\n",
    "             \"Context:\\n{context}\\n\\nTools:\\n{tool_result}\\n\\nQuestion: {question}\\n\"\n",
    "             \"Cite sources as [1], [2] if used.\")\n",
    "        ])\n",
    "        generator = rag_prompt | self.chat_llm | StrOutputParser()\n",
    "\n",
    "        def generate_node(state: RAGState) -> RAGState:\n",
    "            # If tools already set the answer, skip generation\n",
    "            if state.get(\"tool_result\"):\n",
    "                return state\n",
    "            context = format_context(state.get(\"docs\", [])) if state.get(\"docs\") else \"\"\n",
    "            tool_text = state.get(\"tool_result\", \"\")\n",
    "            answer = generator.invoke({\n",
    "                \"history\": state.get(\"history\", []),\n",
    "                \"context\": context,\n",
    "                \"tool_result\": tool_text,\n",
    "                \"question\": state[\"question\"],\n",
    "            })\n",
    "            return {**state, \"answer\": answer}\n",
    "\n",
    "        graph = StateGraph(RAGState)\n",
    "        graph.add_node(\"route\", route_node)\n",
    "        graph.add_node(\"tools\", tools_node)\n",
    "        graph.add_node(\"retrieve\", retrieve_node)\n",
    "        graph.add_node(\"grade\", grade_node)\n",
    "        graph.add_node(\"fallback\", fallback_node)\n",
    "        graph.add_node(\"generate\", generate_node)\n",
    "\n",
    "        graph.add_edge(START, \"route\")\n",
    "        graph.add_conditional_edges(\"route\", lambda s: \"tools\" if s[\"route\"] == \"TOOLS\" else \"retrieve\")\n",
    "        graph.add_edge(\"tools\", END)  # tools short-circuit to END\n",
    "        graph.add_edge(\"retrieve\", \"grade\")\n",
    "        graph.add_conditional_edges(\"grade\", lambda s: \"generate\" if s[\"grounded\"] else \"fallback\")\n",
    "        graph.add_edge(\"generate\", END)\n",
    "        graph.add_edge(\"fallback\", END)\n",
    "\n",
    "        self.graph_app = graph.compile()\n",
    "\n",
    "        # ------- Memory wrapper -------\n",
    "        self._session_store = {}\n",
    "\n",
    "        def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "            if session_id not in self._session_store:\n",
    "                self._session_store[session_id] = InMemoryChatMessageHistory()\n",
    "            return self._session_store[session_id]\n",
    "\n",
    "        # Return only final string answer from the graph\n",
    "        answer_only = self.graph_app | RunnableLambda(lambda state: state[\"answer\"])\n",
    "\n",
    "        self.graph_with_memory = RunnableWithMessageHistory(\n",
    "            answer_only,\n",
    "            get_session_history,\n",
    "            input_messages_key=\"question\",\n",
    "            history_messages_key=\"history\",\n",
    "        )\n",
    "\n",
    "        # ------- Back-compat: rag_chain.invoke(question) returns string via graph -------\n",
    "        class _Invoker:\n",
    "            def __init__(self, outer):\n",
    "                self._outer = outer\n",
    "            def invoke(self, question: str, session_id: str = \"default\"):\n",
    "                cfg = {\"configurable\": {\"session_id\": session_id}}\n",
    "                return self._outer.graph_with_memory.invoke({\"session_id\": session_id, \"question\": question}, config=cfg)\n",
    "\n",
    "        self.rag_chain = _Invoker(self)\n",
    "\n",
    "    # Convenience method for callers\n",
    "    def answer_with_graph(self, question: str, session_id: str = \"default\") -> str:\n",
    "        cfg = {\"configurable\": {\"session_id\": session_id}}\n",
    "        return self.graph_with_memory.invoke({\"session_id\": session_id, \"question\": question}, config=cfg)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bot = ChatBot()\n",
    "    try:\n",
    "        q = input(\"Ask me anything: \")\n",
    "    except EOFError:\n",
    "        q = \"What can Sagittarius expect this week?\"\n",
    "    ans = bot.answer_with_graph(q, session_id=\"cli-user\")\n",
    "    print(ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-groq\n",
      "  Downloading langchain_groq-0.3.7-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting groq\n",
      "  Downloading groq-0.31.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-groq) (0.3.74)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from groq) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from groq) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
      "Requirement already satisfied: certifi in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (0.4.13)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (6.0.2)\n",
      "Requirement already satisfied: packaging>=23.2 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain-groq) (2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->groq) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->groq) (0.4.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (3.11.1)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (2.32.4)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (0.23.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /nuvodata/User_data/priti/miniconda3/lib/python3.13/site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain-groq) (2.5.0)\n",
      "Downloading langchain_groq-0.3.7-py3-none-any.whl (16 kB)\n",
      "Downloading groq-0.31.0-py3-none-any.whl (131 kB)\n",
      "Installing collected packages: groq, langchain-groq\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [langchain-groq]\n",
      "\u001b[1A\u001b[2KSuccessfully installed groq-0.31.0 langchain-groq-0.3.7\n"
     ]
    }
   ],
   "source": [
    "# from your conda env / venv\n",
    "!pip install -U langchain-groq groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: <class 'langchain_groq.chat_models.ChatGroq'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq \n",
    "print('OK:', ChatGroq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an Aries, you may face communication and tech disruptions this year due to Mercury retrograde periods, potentially affecting your travel plans and daily routines. However, by the end of November, the sun's focus on your first house will bring opportunities for deeper connections and new experiences.\n"
     ]
    }
   ],
   "source": [
    "#main.py for groq\n",
    "# main.py\n",
    "import os\n",
    "from typing import List, Optional, Literal, TypedDict\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Pinecone + Vector store\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# Embeddings (local HF download; NOT HF inference)\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Data loading / splitting\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# LangChain core\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Advanced retrieval\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "# Tools\n",
    "from langchain_core.tools import tool\n",
    "import hashlib, datetime, re\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Memory (version-agnostic)\n",
    "try:\n",
    "    from langchain_community.chat_message_histories import ChatMessageHistory as InMemoryChatMessageHistory\n",
    "except ImportError:\n",
    "    from langchain_community.chat_message_histories import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# LLMs via Groq (free tier available)\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "class ChatBot:\n",
    "    def __init__(\n",
    "        self,\n",
    "        index_name: str = \"langchain-demo\",\n",
    "        cloud: str = \"aws\",\n",
    "        region: str = \"us-east-1\",\n",
    "    ):\n",
    "        load_dotenv()\n",
    "\n",
    "        # --- Required env ---\n",
    "        pinecone_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "        groq_key = os.getenv(\"GROQ_API_KEY\")\n",
    "        if not pinecone_key:\n",
    "            raise RuntimeError(\"Missing PINECONE_API_KEY in environment.\")\n",
    "        if not groq_key:\n",
    "            raise RuntimeError(\"Missing GROQ_API_KEY in environment.\")\n",
    "\n",
    "        # --- Load & split docs ---\n",
    "        if not os.path.exists(\"./horoscope.txt\"):\n",
    "            raise FileNotFoundError(\"Couldn't find './horoscope.txt'. Make sure the file exists.\")\n",
    "        loader = TextLoader(\"./horoscope.txt\")\n",
    "        documents = loader.load()\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=4)\n",
    "        docs = text_splitter.split_documents(documents)\n",
    "\n",
    "        # --- Embeddings (local; no paid inference) ---\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "        dim = len(self.embeddings.embed_query(\"ping\"))  # expected 768\n",
    "\n",
    "        # --- Pinecone setup ---\n",
    "        pc = Pinecone(api_key=pinecone_key)\n",
    "        self.index_name = index_name\n",
    "\n",
    "        def _idx_name(x):\n",
    "            return x.name if hasattr(x, \"name\") else (x.get(\"name\") if isinstance(x, dict) else None)\n",
    "\n",
    "        existing = {_idx_name(i) for i in pc.list_indexes()}\n",
    "        if self.index_name not in existing:\n",
    "            pc.create_index(\n",
    "                name=self.index_name,\n",
    "                dimension=dim,\n",
    "                metric=\"cosine\",\n",
    "                spec=ServerlessSpec(cloud=cloud, region=region),\n",
    "            )\n",
    "\n",
    "        idx = pc.Index(self.index_name)\n",
    "        stats = idx.describe_index_stats()\n",
    "        namespaces = stats.get(\"namespaces\", {}) or {}\n",
    "        total = sum(ns.get(\"vector_count\", 0) for ns in namespaces.values()) if namespaces else stats.get(\"total_vector_count\", 0) or 0\n",
    "\n",
    "        if total == 0:\n",
    "            self.vectorstore = PineconeVectorStore.from_documents(docs, embedding=self.embeddings, index_name=self.index_name)\n",
    "        else:\n",
    "            self.vectorstore = PineconeVectorStore(index_name=self.index_name, embedding=self.embeddings)\n",
    "\n",
    "        # --- LLMs (Groq) ---\n",
    "        # fast, cheap model for chat + a stricter one for rewriting\n",
    "        self.chat_llm = ChatGroq(model_name=\"llama-3.1-8b-instant\", temperature=0.3, max_tokens=200)\n",
    "        self.rewriter_llm = ChatGroq(model_name=\"llama-3.1-8b-instant\", temperature=0.1, max_tokens=96)\n",
    "\n",
    "        # --- Advanced retrieval (MultiQuery + rerank + compression) ---\n",
    "        base_retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "        mqr = MultiQueryRetriever.from_llm(retriever=base_retriever, llm=self.rewriter_llm)\n",
    "\n",
    "        reranker_model_name = os.getenv(\"RERANKER_MODEL\", \"BAAI/bge-reranker-base\")  # change to -large if you prefer\n",
    "        cross_encoder = HuggingFaceCrossEncoder(model_name=reranker_model_name)\n",
    "        reranker = CrossEncoderReranker(model=cross_encoder, top_n=5)\n",
    "\n",
    "        self.advanced_retriever = ContextualCompressionRetriever(\n",
    "            base_compressor=reranker,\n",
    "            base_retriever=mqr,\n",
    "        )\n",
    "\n",
    "        # --- Helpers ---\n",
    "        self.SIGNS = [\n",
    "            \"aries\",\"taurus\",\"gemini\",\"cancer\",\"leo\",\"virgo\",\n",
    "            \"libra\",\"scorpio\",\"sagittarius\",\"capricorn\",\"aquarius\",\"pisces\"\n",
    "        ]\n",
    "\n",
    "        def extract_sign_from_text(text: str) -> Optional[str]:\n",
    "            t = (text or \"\").lower()\n",
    "            for s in self.SIGNS:\n",
    "                if s in t:\n",
    "                    return s\n",
    "            return None\n",
    "\n",
    "        def filter_docs_by_sign(docs_list: List[Document], sign: Optional[str]) -> List[Document]:\n",
    "            if not sign:\n",
    "                return docs_list\n",
    "            s = sign.lower()\n",
    "            return [d for d in docs_list if s in (d.page_content or \"\").lower()]\n",
    "\n",
    "        def extract_citations(docs_list: List[Document]) -> List[str]:\n",
    "            cites = []\n",
    "            for i, d in enumerate(docs_list, 1):\n",
    "                md = (getattr(d, \"metadata\", {}) or {})\n",
    "                src = md.get(\"source\") or md.get(\"file\") or \"horoscope.txt\"\n",
    "                cites.append(f\"[{i}] {src}\")\n",
    "            return cites\n",
    "\n",
    "        def format_context(docs_list: List[Document]) -> str:\n",
    "            return \"\\n\\n\".join(\n",
    "                getattr(d, \"page_content\", \"\") for d in docs_list if getattr(d, \"page_content\", \"\")\n",
    "            )\n",
    "\n",
    "        self.extract_sign_from_text = extract_sign_from_text\n",
    "        self.filter_docs_by_sign = filter_docs_by_sign\n",
    "        self.extract_citations = extract_citations\n",
    "        self.format_context = format_context\n",
    "\n",
    "        # --- Tools ---\n",
    "        @tool(\"lucky_number\")\n",
    "        def lucky_number(name_or_sign: str) -> str:\n",
    "            \"\"\"Deterministic 'lucky number' (1-9) from a name or zodiac sign.\"\"\"\n",
    "            h = int(hashlib.md5(name_or_sign.strip().lower().encode(\"utf-8\")).hexdigest(), 16)\n",
    "            num = (h % 9) + 1\n",
    "            return f\"Lucky number for '{name_or_sign}': {num}\"\n",
    "\n",
    "        @tool(\"now\")\n",
    "        def now(_: str = \"\") -> str:\n",
    "            \"\"\"Current date/time (ISO format).\"\"\"\n",
    "            return datetime.datetime.now().isoformat(timespec=\"seconds\")\n",
    "\n",
    "        self.lucky_number = lucky_number\n",
    "        self.now = now\n",
    "\n",
    "        # --- LangGraph: route → tools/RAG → grade → fallback/generate (sign-aware) ---\n",
    "        class RAGState(TypedDict, total=False):\n",
    "            session_id: str\n",
    "            question: str\n",
    "            history: List\n",
    "            route: Literal[\"TOOLS\", \"RAG\"]\n",
    "            target_sign: Optional[str]\n",
    "            docs: List[Document]\n",
    "            citations: List[str]\n",
    "            tool_result: Optional[str]\n",
    "            grounded: bool\n",
    "            answer: str\n",
    "\n",
    "        TOOL_KEYWORDS = (\"lucky number\", \"lucky\", \"today\", \"date\", \"time\", \"now\")\n",
    "\n",
    "        def route_node(state: RAGState) -> RAGState:\n",
    "            q = (state.get(\"question\") or \"\")\n",
    "            route = \"TOOLS\" if any(k in q.lower() for k in TOOL_KEYWORDS) else \"RAG\"\n",
    "            return {**state, \"route\": route, \"target_sign\": self.extract_sign_from_text(q)}\n",
    "\n",
    "        def tools_node(state: RAGState) -> RAGState:\n",
    "            q = state[\"question\"]\n",
    "            if \"lucky\" in q.lower():\n",
    "                target = self.extract_sign_from_text(q) or q.strip()\n",
    "                result = self.lucky_number.invoke(target.title())\n",
    "            else:\n",
    "                result = self.now.invoke(\"\")\n",
    "            return {**state, \"tool_result\": result, \"answer\": result, \"docs\": [], \"citations\": [], \"grounded\": True}\n",
    "\n",
    "        def retrieve_node(state: RAGState) -> RAGState:\n",
    "            docs_list = self.advanced_retriever.invoke(state[\"question\"])\n",
    "            sign = state.get(\"target_sign\")\n",
    "            if sign:\n",
    "                docs_list = self.filter_docs_by_sign(docs_list, sign)\n",
    "            return {**state, \"docs\": docs_list, \"citations\": self.extract_citations(docs_list)}\n",
    "\n",
    "        def grade_node(state: RAGState) -> RAGState:\n",
    "            grounded = len(state.get(\"docs\") or []) > 0\n",
    "            return {**state, \"grounded\": grounded}\n",
    "\n",
    "        def fallback_node(state: RAGState) -> RAGState:\n",
    "            sign = state.get(\"target_sign\")\n",
    "            if sign:\n",
    "                msg = f\"I don't have content for {sign.title()} yet. Please add it to the corpus.\"\n",
    "            else:\n",
    "                msg = \"I don't know yet. Please add relevant content to the corpus.\"\n",
    "            return {**state, \"answer\": msg}\n",
    "\n",
    "        rag_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\",\n",
    "             \"You are a concise fortune teller.\\n\"\n",
    "             \"- Use ONLY the provided context and tool output.\\n\"\n",
    "             \"- Answer the user's single question in 1–2 sentences.\\n\"\n",
    "             \"- Do NOT invent new questions or headings.\\n\"\n",
    "             \"- If the context is empty/irrelevant, reply exactly: I don't know.\"),\n",
    "            MessagesPlaceholder(\"history\"),\n",
    "            (\"human\",\n",
    "             \"Context:\\n{context}\\n\\nTool:\\n{tool_result}\\n\\nUser question: {question}\\n\"\n",
    "             \"Citations (optional): {citations}\")\n",
    "        ])\n",
    "        generator = rag_prompt | self.chat_llm | StrOutputParser()\n",
    "\n",
    "        def generate_node(state: RAGState) -> RAGState:\n",
    "            if state.get(\"tool_result\"):\n",
    "                return state\n",
    "            context = self.format_context(state.get(\"docs\", [])) if state.get(\"docs\") else \"\"\n",
    "            tool_text = state.get(\"tool_result\", \"\")\n",
    "            cites = \" \".join(state.get(\"citations\", []))\n",
    "            answer = generator.invoke({\n",
    "                \"history\": state.get(\"history\", []),\n",
    "                \"context\": context,\n",
    "                \"tool_result\": tool_text,\n",
    "                \"question\": state[\"question\"],\n",
    "                \"citations\": cites,\n",
    "            })\n",
    "            answer = re.sub(r'(?mi)^\\s*Question:.*$', '', answer).strip()\n",
    "            return {**state, \"answer\": answer}\n",
    "\n",
    "        graph = StateGraph(RAGState)\n",
    "        graph.add_node(\"route\", route_node)\n",
    "        graph.add_node(\"tools\", tools_node)\n",
    "        graph.add_node(\"retrieve\", retrieve_node)\n",
    "        graph.add_node(\"grade\", grade_node)\n",
    "        graph.add_node(\"fallback\", fallback_node)\n",
    "        graph.add_node(\"generate\", generate_node)\n",
    "\n",
    "        graph.add_edge(START, \"route\")\n",
    "        graph.add_conditional_edges(\"route\", lambda s: \"tools\" if s[\"route\"] == \"TOOLS\" else \"retrieve\")\n",
    "        graph.add_edge(\"tools\", END)\n",
    "        graph.add_edge(\"retrieve\", \"grade\")\n",
    "        graph.add_conditional_edges(\"grade\", lambda s: \"generate\" if s[\"grounded\"] else \"fallback\")\n",
    "        graph.add_edge(\"generate\", END)\n",
    "        graph.add_edge(\"fallback\", END)\n",
    "\n",
    "        self.graph_app = graph.compile()\n",
    "\n",
    "        # --- Memory wrapper ---\n",
    "        self._session_store = {}\n",
    "        def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "            if session_id not in self._session_store:\n",
    "                self._session_store[session_id] = InMemoryChatMessageHistory()\n",
    "            return self._session_store[session_id]\n",
    "\n",
    "        answer_only = self.graph_app | RunnableLambda(lambda state: state[\"answer\"])\n",
    "        self.graph_with_memory = RunnableWithMessageHistory(\n",
    "            answer_only,\n",
    "            get_session_history,\n",
    "            input_messages_key=\"question\",\n",
    "            history_messages_key=\"history\",\n",
    "        )\n",
    "\n",
    "        # Back-compat for Streamlit\n",
    "        class _Invoker:\n",
    "            def __init__(self, outer):\n",
    "                self._outer = outer\n",
    "            def invoke(self, question: str, session_id: str = \"default\"):\n",
    "                cfg = {\"configurable\": {\"session_id\": session_id}}\n",
    "                return self._outer.graph_with_memory.invoke({\"session_id\": session_id, \"question\": question}, config=cfg)\n",
    "        self.rag_chain = _Invoker(self)\n",
    "\n",
    "    # Convenience method\n",
    "    def answer_with_graph(self, question: str, session_id: str = \"default\") -> str:\n",
    "        cfg = {\"configurable\": {\"session_id\": session_id}}\n",
    "        return self.graph_with_memory.invoke({\"session_id\": session_id, \"question\": question}, config=cfg)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bot = ChatBot()\n",
    "    try:\n",
    "        q = input(\"Ask me anything: \")\n",
    "    except EOFError:\n",
    "        q = \"What can Sagittarius expect this week?\"\n",
    "    ans = bot.answer_with_graph(q, session_id=\"cli-user\")\n",
    "    print(ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can expect a mix of challenges and opportunities in 2025, with Mercury retrograde periods potentially disrupting your travel plans and communication. However, the Sun's focus on your first house from November to December will bring new connections and experiences, helping you discover fresh perspectives and expand your horizons.\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "import os\n",
    "import re\n",
    "from typing import List, Optional, Literal, TypedDict\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Pinecone + Vector store\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# Embeddings (local HF download; NOT HF inference)\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Data loading / splitting\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# LangChain core\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Advanced retrieval\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "# Tools\n",
    "from langchain_core.tools import tool\n",
    "import hashlib, datetime\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Memory (version-agnostic)\n",
    "try:\n",
    "    from langchain_community.chat_message_histories import ChatMessageHistory as InMemoryChatMessageHistory\n",
    "except ImportError:\n",
    "    from langchain_community.chat_message_histories import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# LLMs via Groq\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "class ChatBot:\n",
    "    def __init__(\n",
    "        self,\n",
    "        index_name: str = \"langchain-demo-signed\",   # clean index for sign metadata\n",
    "        cloud: str = \"aws\",\n",
    "        region: str = \"us-east-1\",\n",
    "    ):\n",
    "        load_dotenv()\n",
    "\n",
    "        # --- Required env ---\n",
    "        pinecone_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "        groq_key = os.getenv(\"GROQ_API_KEY\")\n",
    "        if not pinecone_key:\n",
    "            raise RuntimeError(\"Missing PINECONE_API_KEY in environment.\")\n",
    "        if not groq_key:\n",
    "            raise RuntimeError(\"Missing GROQ_API_KEY in environment.\")\n",
    "\n",
    "        # --- Load & split docs ---\n",
    "        if not os.path.exists(\"./horoscope.txt\"):\n",
    "            raise FileNotFoundError(\"Couldn't find './horoscope.txt'. Make sure the file exists.\")\n",
    "        loader = TextLoader(\"./horoscope.txt\")\n",
    "        documents = loader.load()\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=4)\n",
    "        docs = text_splitter.split_documents(documents)\n",
    "\n",
    "        # --- Tag each chunk with a dominant zodiac sign in metadata (NO NULLS) ---\n",
    "        SIGN_LIST = [\n",
    "            \"aries\",\"taurus\",\"gemini\",\"cancer\",\"leo\",\"virgo\",\n",
    "            \"libra\",\"scorpio\",\"sagittarius\",\"capricorn\",\"aquarius\",\"pisces\"\n",
    "        ]\n",
    "        sign_re = re.compile(r\"\\b(\" + \"|\".join(SIGN_LIST) + r\")\\b\", flags=re.I)\n",
    "        for d in docs:\n",
    "            text = (d.page_content or \"\")\n",
    "            found = [s.lower() for s in sign_re.findall(text)]\n",
    "            dominant = max(set(found), key=found.count) if found else None\n",
    "            d.metadata = (d.metadata or {})\n",
    "            # Only set the key if we actually have a value (avoid null)\n",
    "            if dominant:\n",
    "                d.metadata[\"sign\"] = dominant  # e.g., \"sagittarius\"\n",
    "            else:\n",
    "                d.metadata.pop(\"sign\", None)\n",
    "            d.metadata.setdefault(\"source\", \"horoscope.txt\")\n",
    "\n",
    "        # --- Embeddings (local; no paid inference) ---\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "        dim = len(self.embeddings.embed_query(\"ping\"))  # expected 768\n",
    "\n",
    "        # --- Pinecone setup ---\n",
    "        pc = Pinecone(api_key=pinecone_key)\n",
    "        self.index_name = index_name\n",
    "\n",
    "        def _idx_name(x):\n",
    "            return x.name if hasattr(x, \"name\") else (x.get(\"name\") if isinstance(x, dict) else None)\n",
    "\n",
    "        existing = {_idx_name(i) for i in pc.list_indexes()}\n",
    "        if self.index_name not in existing:\n",
    "            pc.create_index(\n",
    "                name=self.index_name,\n",
    "                dimension=dim,\n",
    "                metric=\"cosine\",\n",
    "                spec=ServerlessSpec(cloud=cloud, region=region),\n",
    "            )\n",
    "\n",
    "        idx = pc.Index(self.index_name)\n",
    "        stats = idx.describe_index_stats()\n",
    "        namespaces = stats.get(\"namespaces\", {}) or {}\n",
    "        total = sum(ns.get(\"vector_count\", 0) for ns in namespaces.values()) if namespaces else stats.get(\"total_vector_count\", 0) or 0\n",
    "\n",
    "        # If index is empty, upsert with sign metadata; otherwise connect\n",
    "        if total == 0:\n",
    "            self.vectorstore = PineconeVectorStore.from_documents(\n",
    "                docs, embedding=self.embeddings, index_name=self.index_name\n",
    "            )\n",
    "        else:\n",
    "            self.vectorstore = PineconeVectorStore(index_name=self.index_name, embedding=self.embeddings)\n",
    "\n",
    "        # --- LLMs (Groq) ---\n",
    "        self.chat_llm = ChatGroq(model_name=\"llama-3.1-8b-instant\", temperature=0.3, max_tokens=200)\n",
    "        self.rewriter_llm = ChatGroq(model_name=\"llama-3.1-8b-instant\", temperature=0.1, max_tokens=96)\n",
    "\n",
    "        # --- Advanced retrieval (MultiQuery + rerank + compression) ---\n",
    "        base_retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "        mqr = MultiQueryRetriever.from_llm(retriever=base_retriever, llm=self.rewriter_llm)\n",
    "\n",
    "        reranker_model_name = os.getenv(\"RERANKER_MODEL\", \"BAAI/bge-reranker-base\")\n",
    "        cross_encoder = HuggingFaceCrossEncoder(model_name=reranker_model_name)\n",
    "        reranker = CrossEncoderReranker(model=cross_encoder, top_n=5)\n",
    "\n",
    "        self.advanced_retriever = ContextualCompressionRetriever(\n",
    "            base_compressor=reranker,\n",
    "            base_retriever=mqr,\n",
    "        )\n",
    "\n",
    "        # --- Helpers ---\n",
    "        self.SIGNS = SIGN_LIST\n",
    "\n",
    "        def extract_sign_from_text(text: str) -> Optional[str]:\n",
    "            t = (text or \"\").lower()\n",
    "            for s in self.SIGNS:\n",
    "                if s in t:\n",
    "                    return s\n",
    "            return None\n",
    "\n",
    "        # STRICT: keep only docs whose metadata[\"sign\"] matches the target sign\n",
    "        def filter_docs_by_sign(docs_list: List[Document], sign: Optional[str]) -> List[Document]:\n",
    "            if not sign:\n",
    "                return docs_list\n",
    "            s = sign.lower()\n",
    "            keep: List[Document] = []\n",
    "            for d in docs_list:\n",
    "                md_sign = ((d.metadata or {}).get(\"sign\") or \"\").lower()\n",
    "                if md_sign == s:\n",
    "                    keep.append(d)\n",
    "            return keep\n",
    "\n",
    "        def extract_citations(docs_list: List[Document]) -> List[str]:\n",
    "            cites = []\n",
    "            for i, d in enumerate(docs_list, 1):\n",
    "                md = (getattr(d, \"metadata\", {}) or {})\n",
    "                src = md.get(\"source\") or md.get(\"file\") or \"horoscope.txt\"\n",
    "                cites.append(f\"[{i}] {src}\")\n",
    "            return cites\n",
    "\n",
    "        def format_context(docs_list: List[Document]) -> str:\n",
    "            return \"\\n\\n\".join(\n",
    "                getattr(d, \"page_content\", \"\") for d in docs_list if getattr(d, \"page_content\", \"\")\n",
    "            )\n",
    "\n",
    "        self.extract_sign_from_text = extract_sign_from_text\n",
    "        self.filter_docs_by_sign = filter_docs_by_sign\n",
    "        self.extract_citations = extract_citations\n",
    "        self.format_context = format_context\n",
    "\n",
    "        # --- Tools ---\n",
    "        @tool(\"lucky_number\")\n",
    "        def lucky_number(name_or_sign: str) -> str:\n",
    "            \"\"\"Deterministic 'lucky number' (1-9) from a name or zodiac sign.\"\"\"\n",
    "            h = int(hashlib.md5(name_or_sign.strip().lower().encode(\"utf-8\")).hexdigest(), 16)\n",
    "            num = (h % 9) + 1\n",
    "            return f\"Lucky number for '{name_or_sign}': {num}\"\n",
    "\n",
    "        @tool(\"now\")\n",
    "        def now(_: str = \"\") -> str:\n",
    "            \"\"\"Current date/time (ISO format).\"\"\"\n",
    "            return datetime.datetime.now().isoformat(timespec=\"seconds\")\n",
    "\n",
    "        self.lucky_number = lucky_number\n",
    "        self.now = now\n",
    "\n",
    "        # --- LangGraph: route → tools/RAG → grade → fallback/generate (sign-aware) ---\n",
    "        class RAGState(TypedDict, total=False):\n",
    "            session_id: str\n",
    "            question: str\n",
    "            history: List\n",
    "            route: Literal[\"TOOLS\", \"RAG\"]\n",
    "            target_sign: Optional[str]\n",
    "            docs: List[Document]\n",
    "            citations: List[str]\n",
    "            tool_result: Optional[str]\n",
    "            grounded: bool\n",
    "            answer: str\n",
    "\n",
    "        TOOL_KEYWORDS = (\"lucky number\", \"lucky\", \"today\", \"date\", \"time\", \"now\")\n",
    "\n",
    "        def route_node(state: RAGState) -> RAGState:\n",
    "            q = (state.get(\"question\") or \"\")\n",
    "            route = \"TOOLS\" if any(k in q.lower() for k in TOOL_KEYWORDS) else \"RAG\"\n",
    "            return {**state, \"route\": route, \"target_sign\": self.extract_sign_from_text(q)}\n",
    "\n",
    "        def tools_node(state: RAGState) -> RAGState:\n",
    "            q = state[\"question\"]\n",
    "            if \"lucky\" in q.lower():\n",
    "                target = self.extract_sign_from_text(q) or q.strip()\n",
    "                result = self.lucky_number.invoke(target.title())\n",
    "            else:\n",
    "                result = self.now.invoke(\"\")\n",
    "            return {**state, \"tool_result\": result, \"answer\": result, \"docs\": [], \"citations\": [], \"grounded\": True}\n",
    "\n",
    "        def retrieve_node(state: RAGState) -> RAGState:\n",
    "            q = state[\"question\"]\n",
    "            sign = state.get(\"target_sign\")\n",
    "            biased_query = f\"[{sign}] {q}\" if sign else q  # bias retrieval toward the sign\n",
    "            docs_list = self.advanced_retriever.invoke(biased_query)\n",
    "            if sign:\n",
    "                docs_list = self.filter_docs_by_sign(docs_list, sign)\n",
    "            return {**state, \"docs\": docs_list, \"citations\": self.extract_citations(docs_list)}\n",
    "\n",
    "        def grade_node(state: RAGState) -> RAGState:\n",
    "            grounded = len(state.get(\"docs\") or []) > 0\n",
    "            return {**state, \"grounded\": grounded}\n",
    "\n",
    "        def fallback_node(state: RAGState) -> RAGState:\n",
    "            sign = state.get(\"target_sign\")\n",
    "            if sign:\n",
    "                msg = f\"I don't have content for {sign.title()} yet. Please add it to the corpus.\"\n",
    "            else:\n",
    "                msg = \"I don't know yet. Please add relevant content to the corpus.\"\n",
    "            return {**state, \"answer\": msg}\n",
    "\n",
    "        rag_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\",\n",
    "             \"You are a concise fortune teller.\\n\"\n",
    "             \"- Only answer about the user's sign: {target_sign} (if provided).\\n\"\n",
    "             \"- Use ONLY the provided context and tool output.\\n\"\n",
    "             \"- Answer the user's single question in 1–2 sentences.\\n\"\n",
    "             \"- Do NOT invent new questions or headings.\\n\"\n",
    "             \"- If the context is empty or about a different sign, reply exactly: I don't know.\"),\n",
    "            MessagesPlaceholder(\"history\"),\n",
    "            (\"human\",\n",
    "             \"Context:\\n{context}\\n\\nTool:\\n{tool_result}\\n\\nUser question: {question}\\n\"\n",
    "             \"Citations (optional): {citations}\")\n",
    "        ])\n",
    "        generator = rag_prompt | self.chat_llm | StrOutputParser()\n",
    "\n",
    "        def generate_node(state: RAGState) -> RAGState:\n",
    "            if state.get(\"tool_result\"):\n",
    "                return state\n",
    "            context = self.format_context(state.get(\"docs\", [])) if state.get(\"docs\") else \"\"\n",
    "            tool_text = state.get(\"tool_result\", \"\")\n",
    "            cites = \" \".join(state.get(\"citations\", []))\n",
    "            answer = generator.invoke({\n",
    "                \"history\": state.get(\"history\", []),\n",
    "                \"context\": context,\n",
    "                \"tool_result\": tool_text,\n",
    "                \"question\": state[\"question\"],\n",
    "                \"citations\": cites,\n",
    "                \"target_sign\": (state.get(\"target_sign\") or \"\"),\n",
    "            })\n",
    "            answer = re.sub(r'(?mi)^\\s*Question:.*$', '', answer).strip()\n",
    "            return {**state, \"answer\": answer}\n",
    "\n",
    "        graph = StateGraph(RAGState)\n",
    "        graph.add_node(\"route\", route_node)\n",
    "        graph.add_node(\"tools\", tools_node)\n",
    "        graph.add_node(\"retrieve\", retrieve_node)\n",
    "        graph.add_node(\"grade\", grade_node)\n",
    "        graph.add_node(\"fallback\", fallback_node)\n",
    "        graph.add_node(\"generate\", generate_node)\n",
    "\n",
    "        graph.add_edge(START, \"route\")\n",
    "        graph.add_conditional_edges(\"route\", lambda s: \"tools\" if s[\"route\"] == \"TOOLS\" else \"retrieve\")\n",
    "        graph.add_edge(\"tools\", END)\n",
    "        graph.add_edge(\"retrieve\", \"grade\")\n",
    "        graph.add_conditional_edges(\"grade\", lambda s: \"generate\" if s[\"grounded\"] else \"fallback\")\n",
    "        graph.add_edge(\"generate\", END)\n",
    "        graph.add_edge(\"fallback\", END)\n",
    "\n",
    "        self.graph_app = graph.compile()\n",
    "\n",
    "        # --- Memory wrapper ---\n",
    "        self._session_store = {}\n",
    "        def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "            if session_id not in self._session_store:\n",
    "                self._session_store[session_id] = InMemoryChatMessageHistory()\n",
    "            return self._session_store[session_id]\n",
    "\n",
    "        answer_only = self.graph_app | RunnableLambda(lambda state: state[\"answer\"])\n",
    "        self.graph_with_memory = RunnableWithMessageHistory(\n",
    "            answer_only,\n",
    "            get_session_history,\n",
    "            input_messages_key=\"question\",\n",
    "            history_messages_key=\"history\",\n",
    "        )\n",
    "\n",
    "        # Back-compat for Streamlit\n",
    "        class _Invoker:\n",
    "            def __init__(self, outer):\n",
    "                self._outer = outer\n",
    "            def invoke(self, question: str, session_id: str = \"default\"):\n",
    "                cfg = {\"configurable\": {\"session_id\": session_id}}\n",
    "                return self._outer.graph_with_memory.invoke({\"session_id\": session_id, \"question\": question}, config=cfg)\n",
    "        self.rag_chain = _Invoker(self)\n",
    "\n",
    "    # Convenience method\n",
    "    def answer_with_graph(self, question: str, session_id: str = \"default\") -> str:\n",
    "        cfg = {\"configurable\": {\"session_id\": session_id}}\n",
    "        return self.graph_with_memory.invoke({\"session_id\": session_id, \"question\": question}, config=cfg)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bot = ChatBot()\n",
    "    try:\n",
    "        q = input(\"Ask me anything: \")\n",
    "    except EOFError:\n",
    "        q = \"What can Sagittarius expect this week?\"\n",
    "    ans = bot.answer_with_graph(q, session_id=\"cli-user\")\n",
    "    print(ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have content for Aries yet. Please add it to the corpus.\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "import os\n",
    "import re\n",
    "from typing import List, Optional, Literal, TypedDict\n",
    "from collections import Counter\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Pinecone + Vector store\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# Embeddings (local HF download; NOT HF inference)\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Data loading / splitting\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# LangChain core\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Advanced retrieval\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "# Tools\n",
    "from langchain_core.tools import tool\n",
    "import hashlib, datetime\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Memory (version-agnostic)\n",
    "try:\n",
    "    from langchain_community.chat_message_histories import ChatMessageHistory as InMemoryChatMessageHistory\n",
    "except ImportError:\n",
    "    from langchain_community.chat_message_histories import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# LLMs via Groq\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "class ChatBot:\n",
    "    def __init__(\n",
    "        self,\n",
    "        index_name: str = \"langchain-demo-signed-v2\",  # NEW name to force clean reindex\n",
    "        cloud: str = \"aws\",\n",
    "        region: str = \"us-east-1\",\n",
    "    ):\n",
    "        load_dotenv()\n",
    "\n",
    "        # --- Required env ---\n",
    "        pinecone_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "        groq_key = os.getenv(\"GROQ_API_KEY\")\n",
    "        if not pinecone_key:\n",
    "            raise RuntimeError(\"Missing PINECONE_API_KEY in environment.\")\n",
    "        if not groq_key:\n",
    "            raise RuntimeError(\"Missing GROQ_API_KEY in environment.\")\n",
    "\n",
    "        # --- Load base docs ---\n",
    "        if not os.path.exists(\"./horoscope.txt\"):\n",
    "            raise FileNotFoundError(\"Couldn't find './horoscope.txt'. Make sure the file exists.\")\n",
    "        loader = TextLoader(\"./horoscope.txt\")\n",
    "        base_docs = loader.load()  # list[Document]\n",
    "\n",
    "        # --- Tag each BASE document once with its dominant sign (no nulls) ---\n",
    "        SIGN_LIST = [\n",
    "            \"aries\",\"taurus\",\"gemini\",\"cancer\",\"leo\",\"virgo\",\n",
    "            \"libra\",\"scorpio\",\"sagittarius\",\"capricorn\",\"aquarius\",\"pisces\"\n",
    "        ]\n",
    "        sign_re = re.compile(r\"\\b(\" + \"|\".join(SIGN_LIST) + r\")\\b\", flags=re.I)\n",
    "\n",
    "        for d in base_docs:\n",
    "            text = d.page_content or \"\"\n",
    "            found = [s.lower() for s in sign_re.findall(text)]\n",
    "            d.metadata = (d.metadata or {})\n",
    "            if found:\n",
    "                dominant = Counter(found).most_common(1)[0][0]\n",
    "                d.metadata[\"sign\"] = dominant      # e.g., \"sagittarius\"\n",
    "            else:\n",
    "                d.metadata.pop(\"sign\", None)       # ensure no null/None\n",
    "            d.metadata.setdefault(\"source\", \"horoscope.txt\")\n",
    "\n",
    "        # --- Split AFTER tagging so chunks inherit metadata['sign'] ---\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=4)\n",
    "        docs = text_splitter.split_documents(base_docs)\n",
    "\n",
    "        # --- Embeddings ---\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "        dim = len(self.embeddings.embed_query(\"ping\"))  # expected 768\n",
    "\n",
    "        # --- Pinecone setup ---\n",
    "        pc = Pinecone(api_key=pinecone_key)\n",
    "        self.index_name = index_name\n",
    "\n",
    "        def _idx_name(x):\n",
    "            return x.name if hasattr(x, \"name\") else (x.get(\"name\") if isinstance(x, dict) else None)\n",
    "\n",
    "        existing = {_idx_name(i) for i in pc.list_indexes()}\n",
    "        if self.index_name not in existing:\n",
    "            pc.create_index(\n",
    "                name=self.index_name,\n",
    "                dimension=dim,\n",
    "                metric=\"cosine\",\n",
    "                spec=ServerlessSpec(cloud=cloud, region=region),\n",
    "            )\n",
    "\n",
    "        idx = pc.Index(self.index_name)\n",
    "        stats = idx.describe_index_stats()\n",
    "        namespaces = stats.get(\"namespaces\", {}) or {}\n",
    "        total = sum(ns.get(\"vector_count\", 0) for ns in namespaces.values()) if namespaces else stats.get(\"total_vector_count\", 0) or 0\n",
    "\n",
    "        # If index is empty, upsert; otherwise connect\n",
    "        if total == 0:\n",
    "            self.vectorstore = PineconeVectorStore.from_documents(\n",
    "                docs, embedding=self.embeddings, index_name=self.index_name\n",
    "            )\n",
    "        else:\n",
    "            self.vectorstore = PineconeVectorStore(index_name=self.index_name, embedding=self.embeddings)\n",
    "\n",
    "        # --- LLMs (Groq) ---\n",
    "        self.chat_llm = ChatGroq(model_name=\"llama-3.1-8b-instant\", temperature=0.3, max_tokens=200)\n",
    "        self.rewriter_llm = ChatGroq(model_name=\"llama-3.1-8b-instant\", temperature=0.1, max_tokens=96)\n",
    "\n",
    "        # --- Advanced retrieval (MultiQuery + rerank + compression) ---\n",
    "        base_retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "        mqr = MultiQueryRetriever.from_llm(retriever=base_retriever, llm=self.rewriter_llm)\n",
    "\n",
    "        reranker_model_name = os.getenv(\"RERANKER_MODEL\", \"BAAI/bge-reranker-base\")\n",
    "        cross_encoder = HuggingFaceCrossEncoder(model_name=reranker_model_name)\n",
    "        reranker = CrossEncoderReranker(model=cross_encoder, top_n=5)\n",
    "\n",
    "        self.advanced_retriever = ContextualCompressionRetriever(\n",
    "            base_compressor=reranker,\n",
    "            base_retriever=mqr,\n",
    "        )\n",
    "\n",
    "        # --- Helpers ---\n",
    "        self.SIGNS = SIGN_LIST\n",
    "\n",
    "        def extract_sign_from_text(text: str) -> Optional[str]:\n",
    "            t = (text or \"\").lower()\n",
    "            for s in self.SIGNS:\n",
    "                if s in t:\n",
    "                    return s\n",
    "            return None\n",
    "\n",
    "        # STRICT: keep only docs whose metadata[\"sign\"] matches the target sign\n",
    "        def filter_docs_by_sign(docs_list: List[Document], sign: Optional[str]) -> List[Document]:\n",
    "            if not sign:\n",
    "                return docs_list\n",
    "            s = sign.lower()\n",
    "            keep: List[Document] = []\n",
    "            for d in docs_list:\n",
    "                md_sign = ((d.metadata or {}).get(\"sign\") or \"\").lower()\n",
    "                if md_sign == s:\n",
    "                    keep.append(d)\n",
    "            return keep\n",
    "\n",
    "        def extract_citations(docs_list: List[Document]) -> List[str]:\n",
    "            cites = []\n",
    "            for i, d in enumerate(docs_list, 1):\n",
    "                md = (getattr(d, \"metadata\", {}) or {})\n",
    "                src = md.get(\"source\") or md.get(\"file\") or \"horoscope.txt\"\n",
    "                cites.append(f\"[{i}] {src}\")\n",
    "            return cites\n",
    "\n",
    "        def format_context(docs_list: List[Document]) -> str:\n",
    "            return \"\\n\\n\".join(\n",
    "                getattr(d, \"page_content\", \"\") for d in docs_list if getattr(d, \"page_content\", \"\")\n",
    "            )\n",
    "\n",
    "        self.extract_sign_from_text = extract_sign_from_text\n",
    "        self.filter_docs_by_sign = filter_docs_by_sign\n",
    "        self.extract_citations = extract_citations\n",
    "        self.format_context = format_context\n",
    "\n",
    "        # --- Tools ---\n",
    "        @tool(\"lucky_number\")\n",
    "        def lucky_number(name_or_sign: str) -> str:\n",
    "            \"\"\"Deterministic 'lucky number' (1-9) from a name or zodiac sign.\"\"\"\n",
    "            h = int(hashlib.md5(name_or_sign.strip().lower().encode(\"utf-8\")).hexdigest(), 16)\n",
    "            num = (h % 9) + 1\n",
    "            return f\"Lucky number for '{name_or_sign}': {num}\"\n",
    "\n",
    "        @tool(\"now\")\n",
    "        def now(_: str = \"\") -> str:\n",
    "            \"\"\"Current date/time (ISO format).\"\"\"\n",
    "            return datetime.datetime.now().isoformat(timespec=\"seconds\")\n",
    "\n",
    "        self.lucky_number = lucky_number\n",
    "        self.now = now\n",
    "\n",
    "        # --- LangGraph: route → tools/RAG → grade → fallback/generate (sign-aware) ---\n",
    "        class RAGState(TypedDict, total=False):\n",
    "            session_id: str\n",
    "            question: str\n",
    "            history: List\n",
    "            route: Literal[\"TOOLS\", \"RAG\"]\n",
    "            target_sign: Optional[str]\n",
    "            docs: List[Document]\n",
    "            citations: List[str]\n",
    "            tool_result: Optional[str]\n",
    "            grounded: bool\n",
    "            answer: str\n",
    "\n",
    "        TOOL_KEYWORDS = (\"lucky number\", \"lucky\", \"today\", \"date\", \"time\", \"now\")\n",
    "\n",
    "        def route_node(state: RAGState) -> RAGState:\n",
    "            q = (state.get(\"question\") or \"\")\n",
    "            route = \"TOOLS\" if any(k in q.lower() for k in TOOL_KEYWORDS) else \"RAG\"\n",
    "            return {**state, \"route\": route, \"target_sign\": self.extract_sign_from_text(q)}\n",
    "\n",
    "        def tools_node(state: RAGState) -> RAGState:\n",
    "            q = state[\"question\"]\n",
    "            if \"lucky\" in q.lower():\n",
    "                target = self.extract_sign_from_text(q) or q.strip()\n",
    "                result = self.lucky_number.invoke(target.title())\n",
    "            else:\n",
    "                result = self.now.invoke(\"\")\n",
    "            return {**state, \"tool_result\": result, \"answer\": result, \"docs\": [], \"citations\": [], \"grounded\": True}\n",
    "\n",
    "        def retrieve_node(state: RAGState) -> RAGState:\n",
    "            q = state[\"question\"]\n",
    "            sign = state.get(\"target_sign\")\n",
    "            biased_query = f\"[{sign}] {q}\" if sign else q  # bias retrieval toward the sign\n",
    "            docs_list = self.advanced_retriever.invoke(biased_query)\n",
    "            if sign:\n",
    "                docs_list = self.filter_docs_by_sign(docs_list, sign)\n",
    "            return {**state, \"docs\": docs_list, \"citations\": self.extract_citations(docs_list)}\n",
    "\n",
    "        def grade_node(state: RAGState) -> RAGState:\n",
    "            grounded = len(state.get(\"docs\") or []) > 0\n",
    "            return {**state, \"grounded\": grounded}\n",
    "\n",
    "        def fallback_node(state: RAGState) -> RAGState:\n",
    "            sign = state.get(\"target_sign\")\n",
    "            if sign:\n",
    "                msg = f\"I don't have content for {sign.title()} yet. Please add it to the corpus.\"\n",
    "            else:\n",
    "                msg = \"I don't know yet. Please add relevant content to the corpus.\"\n",
    "            return {**state, \"answer\": msg}\n",
    "\n",
    "        rag_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\",\n",
    "             \"You are a concise fortune teller.\\n\"\n",
    "             \"- Only answer about the user's sign: {target_sign} (if provided).\\n\"\n",
    "             \"- Use ONLY the provided context and tool output.\\n\"\n",
    "             \"- Answer the user's single question in 1–2 sentences.\\n\"\n",
    "             \"- Do NOT invent new questions or headings.\\n\"\n",
    "             \"- If the context is empty or about a different sign, reply exactly: I don't know.\"),\n",
    "            MessagesPlaceholder(\"history\"),\n",
    "            (\"human\",\n",
    "             \"Context:\\n{context}\\n\\nTool:\\n{tool_result}\\n\\nUser question: {question}\\n\"\n",
    "             \"Citations (optional): {citations}\")\n",
    "        ])\n",
    "        generator = rag_prompt | self.chat_llm | StrOutputParser()\n",
    "\n",
    "        def generate_node(state: RAGState) -> RAGState:\n",
    "            # Double-safety: if no docs survived filtering, do NOT generate.\n",
    "            if state.get(\"tool_result\"):\n",
    "                return state\n",
    "            if not state.get(\"docs\"):\n",
    "                sign = state.get(\"target_sign\")\n",
    "                msg = f\"I don't have content for {sign.title()} yet. Please add it to the corpus.\" if sign else \"I don't know yet. Please add relevant content to the corpus.\"\n",
    "                return {**state, \"answer\": msg}\n",
    "\n",
    "            context = self.format_context(state.get(\"docs\", []))\n",
    "            tool_text = state.get(\"tool_result\", \"\")\n",
    "            cites = \" \".join(state.get(\"citations\", []))\n",
    "            answer = generator.invoke({\n",
    "                \"history\": state.get(\"history\", []),\n",
    "                \"context\": context,\n",
    "                \"tool_result\": tool_text,\n",
    "                \"question\": state[\"question\"],\n",
    "                \"citations\": cites,\n",
    "                \"target_sign\": (state.get(\"target_sign\") or \"\"),\n",
    "            })\n",
    "            answer = re.sub(r'(?mi)^\\s*Question:.*$', '', answer).strip()\n",
    "            return {**state, \"answer\": answer}\n",
    "\n",
    "        graph = StateGraph(RAGState)\n",
    "        graph.add_node(\"route\", route_node)\n",
    "        graph.add_node(\"tools\", tools_node)\n",
    "        graph.add_node(\"retrieve\", retrieve_node)\n",
    "        graph.add_node(\"grade\", grade_node)\n",
    "        graph.add_node(\"fallback\", fallback_node)\n",
    "        graph.add_node(\"generate\", generate_node)\n",
    "\n",
    "        graph.add_edge(START, \"route\")\n",
    "        graph.add_conditional_edges(\"route\", lambda s: \"tools\" if s[\"route\"] == \"TOOLS\" else \"retrieve\")\n",
    "        graph.add_edge(\"tools\", END)\n",
    "        graph.add_edge(\"retrieve\", \"grade\")\n",
    "        graph.add_conditional_edges(\"grade\", lambda s: \"generate\" if s[\"grounded\"] else \"fallback\")\n",
    "        graph.add_edge(\"generate\", END)\n",
    "        graph.add_edge(\"fallback\", END)\n",
    "\n",
    "        self.graph_app = graph.compile()\n",
    "\n",
    "        # --- Memory wrapper ---\n",
    "        self._session_store = {}\n",
    "        def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "            if session_id not in self._session_store:\n",
    "                self._session_store[session_id] = InMemoryChatMessageHistory()\n",
    "            return self._session_store[session_id]\n",
    "\n",
    "        answer_only = self.graph_app | RunnableLambda(lambda state: state[\"answer\"])\n",
    "        self.graph_with_memory = RunnableWithMessageHistory(\n",
    "            answer_only,\n",
    "            get_session_history,\n",
    "            input_messages_key=\"question\",\n",
    "            history_messages_key=\"history\",\n",
    "        )\n",
    "\n",
    "        # Back-compat for Streamlit\n",
    "        class _Invoker:\n",
    "            def __init__(self, outer):\n",
    "                self._outer = outer\n",
    "            def invoke(self, question: str, session_id: str = \"default\"):\n",
    "                cfg = {\"configurable\": {\"session_id\": session_id}}\n",
    "                return self._outer.graph_with_memory.invoke({\"session_id\": session_id, \"question\": question}, config=cfg)\n",
    "        self.rag_chain = _Invoker(self)\n",
    "\n",
    "    # Convenience method\n",
    "    def answer_with_graph(self, question: str, session_id: str = \"default\") -> str:\n",
    "        cfg = {\"configurable\": {\"session_id\": session_id}}\n",
    "        return self.graph_with_memory.invoke({\"session_id\": session_id, \"question\": question}, config=cfg)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bot = ChatBot()\n",
    "    try:\n",
    "        q = input(\"Ask me anything: \")\n",
    "    except EOFError:\n",
    "        q = \"What can Sagittarius expect this week?\"\n",
    "    ans = bot.answer_with_graph(q, session_id=\"cli-user\")\n",
    "    print(ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This year, 2025, will bring opportunities for growth and transformation, especially in your relationships and emotional experiences. However, be mindful of Mercury retrograde periods that might disrupt your plans and communication.\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "import os\n",
    "import re\n",
    "from typing import List, Optional, Literal, TypedDict\n",
    "from collections import Counter\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Pinecone + Vector store\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# Embeddings (local HF download; NOT HF inference)\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Data loading / splitting\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# LangChain core\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Advanced retrieval\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "# Tools\n",
    "from langchain_core.tools import tool\n",
    "import hashlib, datetime\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Memory (version-agnostic)\n",
    "try:\n",
    "    from langchain_community.chat_message_histories import ChatMessageHistory as InMemoryChatMessageHistory\n",
    "except ImportError:\n",
    "    from langchain_community.chat_message_histories import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# LLMs via Groq\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "class ChatBot:\n",
    "    def __init__(\n",
    "        self,\n",
    "        index_name: str = \"langchain-demo-signed-v2\",\n",
    "        cloud: str = \"aws\",\n",
    "        region: str = \"us-east-1\",\n",
    "    ):\n",
    "        load_dotenv()\n",
    "\n",
    "        # --- Required env ---\n",
    "        pinecone_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "        groq_key = os.getenv(\"GROQ_API_KEY\")\n",
    "        if not pinecone_key:\n",
    "            raise RuntimeError(\"Missing PINECONE_API_KEY in environment.\")\n",
    "        if not groq_key:\n",
    "            raise RuntimeError(\"Missing GROQ_API_KEY in environment.\")\n",
    "\n",
    "        # --- Load base docs ---\n",
    "        if not os.path.exists(\"./horoscope.txt\"):\n",
    "            raise FileNotFoundError(\"Couldn't find './horoscope.txt'. Make sure the file exists.\")\n",
    "        loader = TextLoader(\"./horoscope.txt\")\n",
    "        base_docs = loader.load()  # list[Document]\n",
    "\n",
    "        # --- Tag each BASE document once with its dominant sign (no nulls) ---\n",
    "        SIGN_LIST = [\n",
    "            \"aries\",\"taurus\",\"gemini\",\"cancer\",\"leo\",\"virgo\",\n",
    "            \"libra\",\"scorpio\",\"sagittarius\",\"capricorn\",\"aquarius\",\"pisces\"\n",
    "        ]\n",
    "        sign_re = re.compile(r\"\\b(\" + \"|\".join(SIGN_LIST) + r\")\\b\", flags=re.I)\n",
    "\n",
    "        for d in base_docs:\n",
    "            text = d.page_content or \"\"\n",
    "            found = [s.lower() for s in sign_re.findall(text)]\n",
    "            d.metadata = (d.metadata or {})\n",
    "            if found:\n",
    "                dominant = Counter(found).most_common(1)[0][0]\n",
    "                d.metadata[\"sign\"] = dominant      # e.g., \"sagittarius\"\n",
    "            else:\n",
    "                d.metadata.pop(\"sign\", None)       # ensure no null/None\n",
    "            d.metadata.setdefault(\"source\", \"horoscope.txt\")\n",
    "\n",
    "        # --- Split AFTER tagging so chunks inherit metadata['sign'] ---\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=4)\n",
    "        docs = text_splitter.split_documents(base_docs)\n",
    "\n",
    "        # --- Embeddings ---\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "        dim = len(self.embeddings.embed_query(\"ping\"))  # expected 768\n",
    "\n",
    "        # --- Pinecone setup ---\n",
    "        pc = Pinecone(api_key=pinecone_key)\n",
    "        self.index_name = index_name\n",
    "\n",
    "        def _idx_name(x):\n",
    "            return x.name if hasattr(x, \"name\") else (x.get(\"name\") if isinstance(x, dict) else None)\n",
    "\n",
    "        existing = {_idx_name(i) for i in pc.list_indexes()}\n",
    "        if self.index_name not in existing:\n",
    "            pc.create_index(\n",
    "                name=self.index_name,\n",
    "                dimension=dim,\n",
    "                metric=\"cosine\",\n",
    "                spec=ServerlessSpec(cloud=cloud, region=region),\n",
    "            )\n",
    "\n",
    "        idx = pc.Index(self.index_name)\n",
    "        stats = idx.describe_index_stats()\n",
    "        namespaces = stats.get(\"namespaces\", {}) or {}\n",
    "        total = sum(ns.get(\"vector_count\", 0) for ns in namespaces.values()) if namespaces else stats.get(\"total_vector_count\", 0) or 0\n",
    "\n",
    "        # If index is empty, upsert; otherwise connect\n",
    "        if total == 0:\n",
    "            self.vectorstore = PineconeVectorStore.from_documents(\n",
    "                docs, embedding=self.embeddings, index_name=self.index_name\n",
    "            )\n",
    "        else:\n",
    "            self.vectorstore = PineconeVectorStore(index_name=self.index_name, embedding=self.embeddings)\n",
    "\n",
    "        # --- LLMs (Groq) ---\n",
    "        self.chat_llm = ChatGroq(model_name=\"llama-3.1-8b-instant\", temperature=0.3, max_tokens=200)\n",
    "        self.rewriter_llm = ChatGroq(model_name=\"llama-3.1-8b-instant\", temperature=0.1, max_tokens=96)\n",
    "\n",
    "        # --- Advanced retrieval (MultiQuery + rerank + compression) ---\n",
    "        base_retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "        mqr = MultiQueryRetriever.from_llm(retriever=base_retriever, llm=self.rewriter_llm)\n",
    "\n",
    "        reranker_model_name = os.getenv(\"RERANKER_MODEL\", \"BAAI/bge-reranker-base\")\n",
    "        cross_encoder = HuggingFaceCrossEncoder(model_name=reranker_model_name)\n",
    "        reranker = CrossEncoderReranker(model=cross_encoder, top_n=5)\n",
    "\n",
    "        self.advanced_retriever = ContextualCompressionRetriever(\n",
    "            base_compressor=reranker,\n",
    "            base_retriever=mqr,\n",
    "        )\n",
    "\n",
    "        # --- Helpers ---\n",
    "        self.SIGNS = SIGN_LIST\n",
    "\n",
    "        def extract_sign_from_text(text: str) -> Optional[str]:\n",
    "            t = (text or \"\").lower()\n",
    "            for s in self.SIGNS:\n",
    "                if s in t:\n",
    "                    return s\n",
    "            return None\n",
    "\n",
    "        # STRICT: keep only docs whose metadata[\"sign\"] matches the target sign\n",
    "        def filter_docs_by_sign(docs_list: List[Document], sign: Optional[str]) -> List[Document]:\n",
    "            if not sign:\n",
    "                return docs_list\n",
    "            s = sign.lower()\n",
    "            keep: List[Document] = []\n",
    "            for d in docs_list:\n",
    "                md_sign = ((d.metadata or {}).get(\"sign\") or \"\").lower()\n",
    "                if md_sign == s:\n",
    "                    keep.append(d)\n",
    "            return keep\n",
    "\n",
    "        def extract_citations(docs_list: List[Document]) -> List[str]:\n",
    "            cites = []\n",
    "            for i, d in enumerate(docs_list, 1):\n",
    "                md = (getattr(d, \"metadata\", {}) or {})\n",
    "                src = md.get(\"source\") or md.get(\"file\") or \"horoscope.txt\"\n",
    "                cites.append(f\"[{i}] {src}\")\n",
    "            return cites\n",
    "\n",
    "        def format_context(docs_list: List[Document]) -> str:\n",
    "            return \"\\n\\n\".join(\n",
    "                getattr(d, \"page_content\", \"\") for d in docs_list if getattr(d, \"page_content\", \"\")\n",
    "            )\n",
    "\n",
    "        def is_general_question(q: str) -> bool:\n",
    "            \"\"\"Heuristic: True for general knowledge/how-to questions (not astrology/tools).\"\"\"\n",
    "            ql = (q or \"\").lower()\n",
    "            if any(s in ql for s in self.SIGNS):\n",
    "                return False\n",
    "            if any(k in ql for k in (\"lucky number\", \"lucky\", \"today\", \"date\", \"time\", \"now\")):\n",
    "                return False\n",
    "            GENERAL_HINTS = (\"capital\", \"recipe\", \"how to\", \"who is\", \"what is\", \"define\",\n",
    "                             \"population\", \"country\", \"city\", \"explain\", \"difference between\")\n",
    "            return any(h in ql for h in GENERAL_HINTS)\n",
    "\n",
    "        self.extract_sign_from_text = extract_sign_from_text\n",
    "        self.filter_docs_by_sign = filter_docs_by_sign\n",
    "        self.extract_citations = extract_citations\n",
    "        self.format_context = format_context\n",
    "        self.is_general_question = is_general_question\n",
    "\n",
    "        # --- Tools ---\n",
    "        @tool(\"lucky_number\")\n",
    "        def lucky_number(name_or_sign: str) -> str:\n",
    "            \"\"\"Deterministic 'lucky number' (1-9) from a name or zodiac sign.\"\"\"\n",
    "            h = int(hashlib.md5(name_or_sign.strip().lower().encode(\"utf-8\")).hexdigest(), 16)\n",
    "            num = (h % 9) + 1\n",
    "            return f\"Lucky number for '{name_or_sign}': {num}\"\n",
    "\n",
    "        @tool(\"now\")\n",
    "        def now(_: str = \"\") -> str:\n",
    "            \"\"\"Current date/time (ISO format).\"\"\"\n",
    "            return datetime.datetime.now().isoformat(timespec=\"seconds\")\n",
    "\n",
    "        self.lucky_number = lucky_number\n",
    "        self.now = now\n",
    "\n",
    "        # --- LangGraph: route → tools/GENERAL/RAG → grade → fallback/generate ---\n",
    "        class RAGState(TypedDict, total=False):\n",
    "            session_id: str\n",
    "            question: str\n",
    "            history: List\n",
    "            route: Literal[\"TOOLS\", \"GENERAL\", \"RAG\"]\n",
    "            target_sign: Optional[str]\n",
    "            docs: List[Document]\n",
    "            citations: List[str]\n",
    "            tool_result: Optional[str]\n",
    "            grounded: bool\n",
    "            answer: str\n",
    "\n",
    "        TOOL_KEYWORDS = (\"lucky number\", \"lucky\", \"today\", \"date\", \"time\", \"now\")\n",
    "\n",
    "        # --- Prompts ---\n",
    "        rag_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\",\n",
    "             \"You are a concise fortune teller.\\n\"\n",
    "             \"- Only answer about the user's sign: {target_sign} (if provided).\\n\"\n",
    "             \"- Use ONLY the provided context and tool output.\\n\"\n",
    "             \"- Answer the user's single question in 1–2 sentences.\\n\"\n",
    "             \"- Do NOT invent new questions or headings.\\n\"\n",
    "             \"- If the context is empty or about a different sign, reply exactly: I don't know.\"),\n",
    "            MessagesPlaceholder(\"history\"),\n",
    "            (\"human\",\n",
    "             \"Context:\\n{context}\\n\\nTool:\\n{tool_result}\\n\\nUser question: {question}\\n\"\n",
    "             \"Citations (optional): {citations}\")\n",
    "        ])\n",
    "        generator = rag_prompt | self.chat_llm | StrOutputParser()\n",
    "\n",
    "        general_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\",\n",
    "             \"You are a helpful assistant. Answer the user clearly and concisely in 2–3 sentences.\"),\n",
    "            MessagesPlaceholder(\"history\"),\n",
    "            (\"human\", \"{question}\")\n",
    "        ])\n",
    "        general_generator = general_prompt | self.chat_llm | StrOutputParser()\n",
    "\n",
    "        # --- Nodes ---\n",
    "        def route_node(state: RAGState) -> RAGState:\n",
    "            q = (state.get(\"question\") or \"\")\n",
    "            if any(k in q.lower() for k in TOOL_KEYWORDS):\n",
    "                route = \"TOOLS\"\n",
    "            elif self.is_general_question(q):\n",
    "                route = \"GENERAL\"\n",
    "            else:\n",
    "                route = \"RAG\"\n",
    "            return {**state, \"route\": route, \"target_sign\": self.extract_sign_from_text(q)}\n",
    "\n",
    "        def tools_node(state: RAGState) -> RAGState:\n",
    "            q = state[\"question\"]\n",
    "            if \"lucky\" in q.lower():\n",
    "                target = self.extract_sign_from_text(q) or q.strip()\n",
    "                result = self.lucky_number.invoke(target.title())\n",
    "            else:\n",
    "                result = self.now.invoke(\"\")\n",
    "            return {**state, \"tool_result\": result, \"answer\": result, \"docs\": [], \"citations\": [], \"grounded\": True}\n",
    "\n",
    "        def general_node(state: RAGState) -> RAGState:\n",
    "            ans = general_generator.invoke({\n",
    "                \"history\": state.get(\"history\", []),\n",
    "                \"question\": state[\"question\"]\n",
    "            })\n",
    "            return {**state, \"answer\": ans, \"docs\": [], \"citations\": [], \"grounded\": False}\n",
    "\n",
    "        def retrieve_node(state: RAGState) -> RAGState:\n",
    "            q = state[\"question\"]\n",
    "            sign = state.get(\"target_sign\")\n",
    "            biased_query = f\"[{sign}] {q}\" if sign else q\n",
    "            docs_list = self.advanced_retriever.invoke(biased_query)\n",
    "            if sign:\n",
    "                docs_list = self.filter_docs_by_sign(docs_list, sign)\n",
    "            return {**state, \"docs\": docs_list, \"citations\": self.extract_citations(docs_list)}\n",
    "\n",
    "        def grade_node(state: RAGState) -> RAGState:\n",
    "            grounded = len(state.get(\"docs\") or []) > 0\n",
    "            return {**state, \"grounded\": grounded}\n",
    "\n",
    "        def fallback_node(state: RAGState) -> RAGState:\n",
    "            # Non-general, horoscope-style query with no matching context\n",
    "            sign = state.get(\"target_sign\")\n",
    "            if sign:\n",
    "                msg = f\"I don't have content for {sign.title()} yet. Please add it to the corpus.\"\n",
    "            else:\n",
    "                msg = \"I don't know yet. Please add relevant content to the corpus.\"\n",
    "            return {**state, \"answer\": msg, \"grounded\": False}\n",
    "\n",
    "        def generate_node(state: RAGState) -> RAGState:\n",
    "            if state.get(\"tool_result\"):\n",
    "                return state\n",
    "            if not state.get(\"docs\"):\n",
    "                return fallback_node(state)\n",
    "\n",
    "            context = self.format_context(state.get(\"docs\", []))\n",
    "            tool_text = state.get(\"tool_result\", \"\")\n",
    "            cites = \" \".join(state.get(\"citations\", []))\n",
    "            answer = generator.invoke({\n",
    "                \"history\": state.get(\"history\", []),\n",
    "                \"context\": context,\n",
    "                \"tool_result\": tool_text,\n",
    "                \"question\": state[\"question\"],\n",
    "                \"citations\": cites,\n",
    "                \"target_sign\": (state.get(\"target_sign\") or \"\"),\n",
    "            })\n",
    "            answer = re.sub(r'(?mi)^\\s*Question:.*$', '', answer).strip()\n",
    "            return {**state, \"answer\": answer}\n",
    "\n",
    "        # --- Graph wiring ---\n",
    "        graph = StateGraph(RAGState)\n",
    "        graph.add_node(\"route\", route_node)\n",
    "        graph.add_node(\"tools\", tools_node)\n",
    "        graph.add_node(\"general\", general_node)\n",
    "        graph.add_node(\"retrieve\", retrieve_node)\n",
    "        graph.add_node(\"grade\", grade_node)\n",
    "        graph.add_node(\"fallback\", fallback_node)\n",
    "        graph.add_node(\"generate\", generate_node)\n",
    "\n",
    "        def _router(s: RAGState) -> str:\n",
    "            return \"tools\" if s[\"route\"] == \"TOOLS\" else (\"general\" if s[\"route\"] == \"GENERAL\" else \"retrieve\")\n",
    "\n",
    "        graph.add_edge(START, \"route\")\n",
    "        graph.add_conditional_edges(\"route\", _router)\n",
    "        graph.add_edge(\"tools\", END)\n",
    "        graph.add_edge(\"general\", END)\n",
    "        graph.add_edge(\"retrieve\", \"grade\")\n",
    "        graph.add_conditional_edges(\"grade\", lambda s: \"generate\" if s[\"grounded\"] else \"fallback\")\n",
    "        graph.add_edge(\"generate\", END)\n",
    "        graph.add_edge(\"fallback\", END)\n",
    "\n",
    "        self.graph_app = graph.compile()\n",
    "\n",
    "        # --- Memory wrapper (ephemeral, in-process) ---\n",
    "        self._session_store = {}\n",
    "        def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "            if session_id not in self._session_store:\n",
    "                self._session_store[session_id] = InMemoryChatMessageHistory()\n",
    "            return self._session_store[session_id]\n",
    "\n",
    "        answer_only = self.graph_app | RunnableLambda(lambda state: state[\"answer\"])\n",
    "        self.graph_with_memory = RunnableWithMessageHistory(\n",
    "            answer_only,\n",
    "            get_session_history,\n",
    "            input_messages_key=\"question\",\n",
    "            history_messages_key=\"history\",\n",
    "        )\n",
    "\n",
    "        # Back-compat for Streamlit\n",
    "        class _Invoker:\n",
    "            def __init__(self, outer):\n",
    "                self._outer = outer\n",
    "            def invoke(self, question: str, session_id: str = \"default\"):\n",
    "                cfg = {\"configurable\": {\"session_id\": session_id}}\n",
    "                return self._outer.graph_with_memory.invoke({\"session_id\": session_id, \"question\": question}, config=cfg)\n",
    "        self.rag_chain = _Invoker(self)\n",
    "\n",
    "    # Convenience method\n",
    "    def answer_with_graph(self, question: str, session_id: str = \"default\") -> str:\n",
    "        cfg = {\"configurable\": {\"session_id\": session_id}}\n",
    "        return self.graph_with_memory.invoke({\"session_id\": session_id, \"question\": question}, config=cfg)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bot = ChatBot()\n",
    "    try:\n",
    "        q = input(\"Ask me anything: \")\n",
    "    except EOFError:\n",
    "        q = \"What can Sagittarius expect this week?\"\n",
    "    ans = bot.answer_with_graph(q, session_id=\"cli-user\")\n",
    "    print(ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[{'type': 'human', 'data': {'content': 'Hi!', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': None, 'example': False}}, {'type': 'ai', 'data': {'content': 'You are a Sagittarius. This year, your journey of discovering new places and meeting people who can offer a fresh perspective on the world and yourself will be highlighted, making it a perfect time to seek deeper connections and expand your horizons.', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': None, 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}}, {'type': 'human', 'data': {'content': 'Remember this.', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': None, 'example': False}}, {'type': 'ai', 'data': {'content': 'This year, your journey of discovering new places and meeting people who can offer a fresh perspective on the world and yourself will be highlighted, making it a perfect time to seek deeper connections and expand your horizons.', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': None, 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}}]\n",
      "[{'type': 'human', 'data': {'content': 'Hi!', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': None, 'example': False}}, {'type': 'ai', 'data': {'content': 'You are a Sagittarius. This year, your journey of discovering new places and meeting people who can offer a fresh perspective on the world and yourself will be highlighted, making it a perfect time to seek deeper connections and expand your horizons.', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': None, 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}}, {'type': 'human', 'data': {'content': 'Remember this.', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': None, 'example': False}}, {'type': 'ai', 'data': {'content': 'This year, your journey of discovering new places and meeting people who can offer a fresh perspective on the world and yourself will be highlighted, making it a perfect time to seek deeper connections and expand your horizons.', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': None, 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}}]\n"
     ]
    }
   ],
   "source": [
    "#chat history\n",
    "from main import ChatBot\n",
    "bot = ChatBot()\n",
    "\n",
    "sid = \"demo-1\"\n",
    "print(bot.list_sessions())                # []\n",
    "bot.answer_with_graph(\"Hi!\", sid)\n",
    "bot.answer_with_graph(\"Remember this.\", sid)\n",
    "\n",
    "print(bot.get_history(sid))               # show messages\n",
    "\n",
    "bot.save_history(sid, \"demo-1.json\")      # persist\n",
    "bot.clear_history(sid)                    # wipe in-RAM\n",
    "bot.load_history(sid, \"demo-1.json\")      # restore\n",
    "print(bot.get_history(sid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The chat memory is kept in RAM, locally, inside your Python process (that _session_store dict).\n",
    "\n",
    "# If you restart Python/Streamlit or the process crashes, that in-RAM memory is gone—unless you save it to a file (save_history(...)).\n",
    "\n",
    "# save_history(...) writes a JSON file on disk (e.g., demo-1.json). load_history(...) pulls it back into RAM.\n",
    "\n",
    "# In your Streamlit app:\n",
    "\n",
    "#     Reset session = new session_id ⇒ starts a fresh memory thread.\n",
    "\n",
    "#     Clear chat button only clears the UI list; to truly wipe memory call clear_history(session_id)\n",
    "# Pinecone stores your vectors in the cloud (persistent).\n",
    "\n",
    "# Groq runs the LLM remotely (stateless)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "priti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
